\documentclass[9pt,twocolumn,twoside,lineno]{pnas-new}
% Use the lineno option to display guide line numbers if required.

\templatetype{pnasresearcharticle} % Choose template 
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} %= Template for a one-column mathematics article
% {pnasinvited} %= Template for a PNAS invited submission

\input{../../common/prelim.tex}

\title{Can Auxiliary Indicators Improve COVID-19 Forecasts and Hotspot
  Prediction?} 

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[a,c,1]{Author One}
\author[b,1,2]{Author Two} 
\author[a]{Author Three}

\affil[a]{Affiliation One}
\affil[b]{Affiliation Two}
\affil[c]{Affiliation Three}

% Please give the surname of the lead author for the running footer
\leadauthor{Lead author last name} 

% Please add a significance statement to explain the relevance of your work
% TODO write a real one. This is the indicator's one.
\significancestatement{To study the COVID-19 pandemic, its effects on society,
  and measures for reducing its spread, researchers need detailed data on the
  course of the pandemic. Standard public health data streams suffer
  inconsistent reporting and frequent, unexpected revisions. They also miss
  other aspects of a population's behavior, worthy of consideration. We
  present an open database of COVID signals in the United States, measured at
  the county level and updated daily. These include traditionally reported COVID
  cases and deaths, and many others: signals based on measures of mobility,
  social distancing, internet search trends, self-reported symptoms, and
  patterns COVID-related activity in de-identified medical insurance claims. The
  database provides all signals in a common, easy-to-use format, empowering both
  public health research and operational decision-making.
}

% Please include corresponding author, author contribution and author
% declaration information 
\authorcontributions{Please provide details of author contributions here.}
\authordeclaration{Please declare any competing interests here.}
\equalauthors{\textsuperscript{1}A.O.(Author One) contributed equally to this
  work with A.T. (Author Two) (remove if not applicable).} 
\correspondingauthor{\textsuperscript{2}To whom correspondence should be
  addressed. E-mail: author.two\@email.com} 

% At least three keywords are required at submission. Please provide three to
% five keywords, separated by the pipe symbol. 
\keywords{Keyword 1 $|$ Keyword 2 $|$ Keyword 3 $|$ ...} 

\begin{abstract}
Please provide an abstract of no more than 250 words in a single paragraph.
Abstracts should explain to the general reader the major contributions of the
article. References in the abstract must be cited in full within the abstract
itself and cited in the text. 
\end{abstract}

\dates{This manuscript was compiled on \today}
\doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}

\begin{document}

\maketitle
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}

\dropcap{T}racking and forecasting indicators from public health reporting
streams---such as confirmed cases and deaths in the COVID-19 pandemic---is
crucial for understanding disease spread, formulating public policy responses,
and anticipating future public health resource needs.  In a companion paper, we
describe our research group's (Delphi's) efforts in curating and maintaining a 
database of real-time indicators that track COVID-19 activity and other relevant
phenomena. The signals (a term we use synonomously with ``indicators'') in this
database are accessible through the COVIDcast API \cite{CovidcastAPI}, with
associated R \cite{CovidcastR} and Python \cite{CovidcastPy} packages for
convenient data fetching and processing tools. In the current paper, we aim to
quantify the utility provided by a core set of these indicators for two
fundamental prediction tasks: probabilistic forecasting of COVID-19 case 
rates and prediction of future COVID-19 case hotspots (defined by the event that
a relative increase in COVID-19 cases exceeds a certain threshold). 

At the outset, we should be clear that our intent in this paper is \textit{not}
to provide an authoritative take on cutting-edge COVID-19 forecasting methods.
Instead, our purpose here is to provide a rigorous, quantitative assessment of
the utility that several auxiliary indicators---such as those derived from
internet surveys or medical insurance claims---provide in tasks that involve
predicting future trends in confirmed COVID-19 cases. To assess such utility in
as simple terms as possible, we center our study in the framework of a basic
autoregressive model (in which COVID cases in the near future are predicted from  
a linear combination of COVID cases in the near past), and ask whether the 
inclusion of an auxiliary indicator as an additional feature in such a model
improves its predictions. 

While forecasting carries a rich literature that offers a wide range of
techniques, see e.g., \cite{Hyndman:2018}, we purposely constrain ourselves to
very simple models, avoiding common enhancements such as order selection,
correction of outliers/anomalies in the data, and inclusion of regularization or
nonlinearities. That said, analyses of forecasts submitted to the COVID-19
Forecast Hub \cite{ForecastHub} by a large community of modelers have shown that
simple, robust models have consistently been among the best-performing over the
pandemic \cite{Cramer:2021}, including time series models similar to those we
consider in what follows.   

In our companion paper, we analyze correlations between various indicators and   
COVID case rates. These correlations are natural summaries of the
contemporaneous association between an indicator and COVID cases, but they fall 
short of delivering a satisfactory answer to the question that motivates the
current article: is the information contained in an indicator demonstrably
useful for the prediction tasks we care about? For such a question, lagged
correlations (e.g., measuring the correlation between an indicator and COVID
case rates several days in the future) move us in the right direction, but still
do not move us all the way there. The question about \textit{utility for 
  prediction} is focused on a much higher standard than simply asking 
about correlations; to be useful in forecast or hotspot models, an indicator
must provide relevant information that is not otherwise contained in past values
of the case rate series itself. We will assess this in the most direct way
possible: by inpsecting the difference in predictive performance of simple
autoregressive models trained with and without access to past values of a
particular indicator.   

There is another, more subtle issue in evaluating predictive utility that 
deserves explicit mention, as it will play a key role in our analysis.
Signals computed from surveillance streams will often be subject to  
latency and/or revision. For example, a signal based on aggregated medical
insurance claims may be available after just a few days, but it can then be
substantially revised over the next several weeks as additional claims are
submitted and/or processed late. Correlations between such a signal and case
rates calculated ``after the fact'' (i.e., computed retrospectively, using the
finalized values of this signal) will not deliver an honest answer to the    
question of whether this signal would have been useful in real time. Instead,
we build predictive models using only the data that would have been available
\textit{as of} the prediction date, and compare the ensuing predictions in terms 
of accuracy. To do so, we leverage Delphi's \texttt{evalcast} R package 
\cite{EvalcastR}, which plugs into the COVIDcast API's data versioning system,  
and facilitates honest backtesting. 

Finally, it is worth noting that examining the importance of additional features
for prediction is a core question in inferential statistics and econometrics,
with work dating back to at least \cite{Granger:1969}. Still today, drawing
rigorous inference based on predictions, without (or with lean) assumptions, is
an active field of research from both the applied and theoretical angles;
see, e.g., \cite{Diebold:2002, McCraken:2007, Diebold:2015, Stokes:2017,
  Lei:2018, Rinaldo:2019, Williamsom:2020, Zhang:2020, Dai:2021, Fryer:2021}.
Our take on this problem is in line with much of this literature; however, in
order to avoid making any explicit assumptions, we do attempt to make explicit
significance statements, and instead, broadly examine the stability of our
conclusions with respect to numerous modes of analysis.   

\section{Methods}

\subsection{Signals and Locations}

We consider prediction of future COVID-19 case rates or case hotspots (to be
defined precisely shortly).  By case rate, we mean the case count per 100,000
people (the standard in epidemiology).  We use reported case data aggregated by 
JHU CSSE \cite{Dong:2020}, which is accessible through the COVIDcast API
\cite{CovidcastAPI}, like the auxiliary indicators that we use to supplement the
basic purely autoregressive models.   

The indicators we focus on provide information not generally available from
standard public health reporting. Among the many auxiliary indicators collected
in the API, we study the following five:  
\begin{itemize}
\item Change Healthcare COVID-like illness (CHNG-CLI): The percentage of
  outpatient visits that are primarily about COVID-related symptoms, based on
  de-identified Change Healthcare claims data.
\item Change Healthcare COVID (CHNG-COVID): The percentage of outpatient visits
  with confirmed COVID-19, based on the same claims data.
\item COVID Tracking and Impact Survey CLI-in-community (CTIS-CLI-in-community): 
  The estimated percentage of the population who knows someone who is sick,
  based on Delphi's surveys of Facebook users.
\item Doctor Visits COVID-like illness (DV-CLI): The same as CHNG-CLI, but
  computed based on de-identified medical insurance claims from other health
  systems partners.  
\item Google Search Trends for anosmia and ageusia (Google-AA): A measure of 
  COVID-19 related Google search volume for queries related to anosmia (loss of
  smell), ageusia (loss of taste), or both.  
\end{itemize}
In short, we choose these indicators because, conceptually speaking, they
measure aspects of an individual's disease progression that would plausibly
precede the occurence of (at worst, co-occur  with) the report of a positive
COVID-19 test, through standard public health reporting streams.

For more details on the five indicators (including how these are precisely
computed from the underlying data streams) we refer to
\url{https://cmu-delphi.github.io/delphi-epidata/api/covidcast_signals.html},
which documents all of the signals in the COVIDcast API, and our companion paper
on the API and database. For CTIS in particular, we refer to our companion paper
on this survey. For the Google Search Trends data set, see
\cite{GoogleSymptoms}, and \cite{Klopfen:2020, Vaira:2020} for articles on the
relevance of anosmia or ageusia (loss of smell or taste) to COVID.

As for geographic resolution, we consider the prediction of COVID-19 case rates
and hotspots aggregated at the level of an individual \textit{hospital referral
  region} (HRR). HRRs correspond to groups of counties in the United States
within the same hospital referral system. The Dartmouth Atlas of Healthcare 
Policy~\cite{DartmouthHRR}, defines these 306 regions based on a number of
characteristics. They are contiguous regions such that most of the hospital
services for the underlying population are performed by hospitals within the
region. Each HRR also contains at least one city where major procedures
(cardiovascular or neurological) are performed. The smallest HRR has a
population of about 120,000. While some are quite large (such as the one
containing Los Angeles, which has about 10 million people), generally HRRs 
are much more homogenous in size than the (approximately) 3200 counties,  
and they serve as a nice middle ground in between counties and states.  

HRRs, by their definition, would be most relevant for forecasting hospital
demand.  We have chosen to focus on cases (forecasting and predicting
hotspots) at the HRR level since the indicators considered should be more 
useful in predicting case activity versus hospital demand, since the latter is
intuitively more closely connected (in time) to the events that are measured by
the given five indicators. Predicting case rates (and hotspots) at the HRR level
is still a reasonable goal in its own right; and moreover, it could be used to
feed predicted case information into downstream hospitalization models.

\subsection{Vintage Training Data}

In this paper, all models are fit with ``vintage'' training data. This means
that for a given prediction date, say, September 1, 2020, we train models 
using only the data that would have been available to us \textit{as of}
September 1 (imagine that we ``rewind'' the clock to September 1, and query the
COVIDcast API to get the latest data it would have had available at that point
in time.)  This is made possible by the COVIDcast API's comprehensive data  
versioning system (described in more detail in our companion paper).  We also
use the \texttt{evalcast} R package \cite{EvalcastR}, which streamlines the
process of training arbitrary prediction models over a sequence of prediction
dates, by constructing the proper sequence of vintage training data sets.   

\begin{figure}[tb!]
\centering
\includegraphics[width=\columnwidth]{fig/vintage.pdf}
\caption{TODO}
\label{fig:vintage}
\end{figure}

Vintage training data means different things, in practice, for different
signals. The three signals based on medical claims, CHNG-CLI, CHNG-COVID, and
DV-CLI, are most often 3-5 days latent, and subject to a considerable but
regular degree of revision or ``backfill'' after their initial publication date.
The survey-based signal, CTIS-CLI-in-community, is 2 days latent, and rarely
undergoes any revision at all.  The target variable itself, reported COVID-19
case rates, is 1 day latent, and exhibits frequent, unpredictable revisions
(sometimes clear anomalies) after initial publication.  Compared to the revision
pattern in the medical claims signals, which are much more systematic in nature,
revisions in case reports can be highly erratic. Big spikes or other anomalies
can occur in the data as reporting backlogs are cleared, changes in case
definitions are made, etc. Groups like JHU CSSE then work tirelessly to correct
such anomalies after first publication (e.g., they will attempt to
back-distribute a spike when a reporting backlog is cleared, by working with a
local authority to figure out how this should best be done), which can result
in very nontrivial and often unpredictable revisions.  See Figure
\ref{fig:vintage} for an example.

Lastly, our treatment of the Google-AA signal is different from the rest.
Because Google's team did not start publishing this signal until early
September, 2020, we do not have true vintage data before then.  However, this
signal is never revised after initial publication (confirmed via personal
communication with the Google team that produces this signal), so we simply use
finalized values for it in our analysis.   

\subsection{Analysis Tasks}

\begin{table*}[t]
  \centering
  \caption{Summary of forecasting and hotspot prediction tasks considered in
    this paper.}
  \begin{tabular}{l p{2.75in} p{2.75in}}
    \toprule
    & \textbf{Forecasting} & \textbf{Hotspot prediction} \\
    \midrule
    Response variable & $Y_{\ell,t}$ (7-day trailing average of COVID-19 case
 incidence rates, per location $\ell$ and time $t$) & $Z_{\ell,t} =
\indicator{Y_{\ell,t} \geq 1.25 \cdot Y_{\ell, t-7}}$ (indicator that $Y_{\ell,t}$ 
grows by more than 25\% relative to the preceding week) \\
    Geographic resolution & Hospital referral region (HRR) & Hospital referral
region (HRR) \\ 
    Forecast period & June 6--December 31, 2020 & June 13--December 31, 2020 \\  
    Model type & Quantile regression & Logistic regression \\
    Evaluation metric & Weighted interval score (WIS) & Area under curve (AUC) \\
    \bottomrule
  \end{tabular}
\label{tab:analysis_tasks}
\end{table*}

To fix notation, let $Y_{\ell,t}$ denote the 7-day trailing average of COVID-19
case incidence rates in location (HRR) $\ell$ and at time (day) $t$.  To be
clear, this is the number of new daily reported cases per 100,000
people, averaged over the 7-day period $t-6, \ldots t$. The first task we
consider---\textit{forecasting}---is to predict $Y_{\ell,t+a}$ for each
``ahead'' value $a=7,\ldots,21$.  The second task---\textit{hotspot
  prediction}---is to predict a binary variable defined in terms of the relative
change of $Y_{\ell,t+a}$ (relative to its value one week prior,
$Y_{\ell,t+a-7}$), again for each $a=7,\ldots,21$.   

Why do we define the response variables via 7-day averaging? The short answer 
is robustness: averaging stabilizes the case time series, and accounts for 
uninteresting artifacts like weekend-weekday differences in the series.  Note
that we can of course equivalently view this (equivalent up to a constant
factor) as predicting the HRR-level COVID-19 case incidence rate \textit{summed} 
over some 7-day period in the future, and predicting a binary variable derived 
from this. 

In what follows, we cover more details on our two analysis
tasks. Table~\ref{tab:analysis_tasks} presents a summary.   

\paragraph{Dynamic Re-Training}

For each prediction date $t$, we use a 21-day trailing window of data to train
our forecast or hotspot prediction models (so, e.g., the trained models will
differ from those at prediction date $t-1$).  This is done to account for
(potential) nonstationarity.  For simplicity, the forecasting and hotspot
prediction models are always trained on data across all HRRs (i.e., the
coefficients in the models do not account for location-specific effects).   

\paragraph{Prediction Period}

In our analysis, we let the prediction date $t$ run over each day in between
early/mid June and December 31, 2020.  The precise start date differs for 
forecasting and hotspots prediction; for each task it was chosen to be the
earliest date at which the data needed to train all models was available, which
ends up being (per our setup, with 21 days of training data, and lagged values
of signals for features as we will detail shortly) June 6, 2020 for forecasting,
and June 13, 2020 for hotspot prediction. (The bottleneck here is the
CTIS-CLI-in-community signal, which does not exist before early April 2020, when
the survey was first launched). The end date was chosen again with a
consideration to align both tasks as best as possible, and because few hotspots
exist post December 31, 2020, due to the general and gradual decline of the
pandemic in 2021.

\paragraph{Forecasting Models}

Recall $Y_{t,\ell}$ denotes the 7-day trailing average of COVID-19 case
incidence rates in location $\ell$ and at time $t$.  Separately for each
$a=7,\ldots,28$, to predict $Y_{\ell,t+a}$ for ahead value $a$, we consider a
simple probabilistic forecasting model of the form:     
\begin{equation}
\label{eq:forecast_ar}
\Quantile_\tau(Y_{\ell,t+a} \,|\, Y_{\ell,s}, s \leq t)  
= \alpha^{a,\tau} + \sum_{j=0}^2 \beta^{a,\tau}_j Y_{\ell,t-7j}.  
\end{equation}
This model uses current case rates, and the case rates 7 and 14 days ago, in
order to predict (the quantiles of) case rates in the future.  We consider a
total of 7 quantile levels (chosen in accordance with the county-level quantile
levels suggested by the COVID-19 Forecast Hub),      
\begin{equation}
\label{eq:quantile_levels}
\tau \in \{0.025, 0.1, 0.25, 0.5, 0.75, 0.9, 0.975 \}.
\end{equation}
We fit \eqref{eq:forecast_ar} using \textit{quantile regression}
\cite{Koenker:1978, Koenker:2005, Koenker:2006} separately for each $\tau$, 
using data from all 306 HRRs, and within each HRR, using the most recent 21 days
of training data.  This gives us 6,426 training samples for each quantile 
regression problem.    

In addition to the above ``pure'' autoregressive model, we consider five
probabilistic forecasting models of the form: 
\begin{multline}
\label{eq:forecast_ar_x}
\Quantile_\tau(Y_{\ell,t+a} \,|\, Y_{\ell,s}, X_{\ell,s}, s \leq t)  
= \\ \alpha^{a,\tau} + \sum_{j=0}^2 \beta^{a,\tau}_j Y_{\ell,t-7j} + 
\sum_{j=0}^2 \gamma^{a,\tau}_j X_{\ell,t-7j},
\end{multline}
where $X_{\ell,t}$ denotes any one of the five auxiliary indicators---CHNG-CLI, 
CHNG-COVID, CTIS-CLI-in-community, DV-CLI, or Google-AA---at location $\ell$ and
time $t$. Note that we apply the same lags (current value, along with the values
7 and 14 days ago) for the auxiliary indicators as we do for the case
rates. Training then proceeds just as before: we use the same 7 quantile levels
in \eqref{eq:quantile_levels}, and fit quantile regression separately for each 
level $\tau$, using data from all 306 HRRs and a trailing window of 21 days of
training data.

At prediction time, in order to deal with possible crossing violations (a
predicted quantile at level $\tau$ exceeds a predicted quantile at level $\tau'
> \tau$), we apply a simple post-hoc sorting.  See Figure \ref{fig:trajectory}
for an example forecast. 

\begin{figure}[tb!]
\centering
\includegraphics[width=\columnwidth]{fig/trajectory.pdf}
\caption{TODO}
\label{fig:trajectory}
\end{figure}

\paragraph{Hotspot Prediction Models}

Define the binary indicator:
$$
Z_{t,\ell} = \indicator{Y^\Delta_{\ell,t} \geq 0.25},
$$
where we use the notation \smash{$Y^\Delta_{\ell,t} = (Y_{\ell,t} - Y_{\ell,
    t-7})/(Y_{\ell,t-7})$}. In other words, $Z_{t,\ell}=1$ if the number of
newly reported cases over the past 7 days has increased by at least 25\%
compared to the preceding week.  When this occurs, we say location $\ell$ is a
\textit{hotspot} at time $t$.  Empirically, this rule labels about 27\% of
location-time pairs as hotspots, during the prediction period (June 13--December
31, 2020).  

We treat hotspot prediction as a binary classification problem and use a setup 
altogether quite similar to the forecasting setup described previously.
Separately for each $a=7,\ldots,28$, to predict $Z_{\ell,t+a}$, we consider a
simple logistic model:
\begin{equation}
\label{eq:hotspot_ar}
\logit(Z_{\ell,t+a} \,|\, Y_{\ell,s}, s \leq t)  
= \alpha^{a,\tau} + \sum_{j=0}^2 \^{a,\tau}_j Y^\Delta_{\ell,t-7j}.  
\end{equation}
In addition to this ``pure'' autoregressive model, we also consider five
logistic models of the form: 
\begin{multline}
\label{eq:hotspot_ar_x}
\logit(Z_{\ell,t+a} \,|\, Y_{\ell,s}, X_{\ell,s}, s \leq t)  
= \\ \alpha^{a,\tau} + \sum_{j=0}^2 \beta^{a,\tau}_j Y^\Delta_{\ell,t-7j} +  
\sum_{j=0}^2 \gamma^{a,\tau}_j X^\Delta_{\ell,t-7j},
\end{multline}
where we use \smash{$X^\Delta_{\ell,t} = (X_{\ell,t} - X_{\ell,
    t-7})/(X_{\ell,t-7})$}, and again $X_{\ell,t}$ stands for any of the five  
auxiliary indicators at location $\ell$ and time $t$.  We fit all models
\eqref{eq:hotspot_ar}, \eqref{eq:hotspot_ar_x} using logistic regression, 
pooling all 306 HRRs and using a 21-day trailing window for the training data.   

An important detail is that in hotspot prediction we remove all data from
training and evaluation where, on average, fewer than 30 cases (this refers to a
count, not a rate) are observed over the preceding 7 days. This avoids having to
make arbitrary calls for a hotspot (or lack thereof) based on small counts.   

\subsection{Evaluation Metrics}

For forecasting, we evaluate the probabilistic forecasts produced by the
quantile models in \eqref{eq:forecast_ar}, \eqref{eq:forecast_ar_x} using 
\textit{weighted interval score} (WIS), a quantile-based scoring rule; see e.g.,
\cite{Gneiting:2007}.  WIS is a proper score, meaning that its expectation
is minimized by the population quantiles of the target variable.  The use of WIS
in COVID-19 forecast scoring is discussed in \cite{Bracher:2021}; WIS is also
the main evaluation metric used in the COVID-19 Forecast Hub.   

WIS is typically defined for quantile-based forecasts where the quantile levels
are symmetric around 0.5.  This is the case for our choice in
\eqref{eq:quantile_levels}.  Let $F$ be a forecaster comprised of predicted
quantiles $q_\tau$ parametrized by a quantile level $\tau$.  In the case of
symmetric quantile levels, this is equivalent to a collection of central
prediction intervals $(\ell_\alpha, u_\alpha)$, parametrized by an exclusion
probability $\alpha$. The WIS of the forecaster $F$, evaluated at the target
variable $Y$, is defined by:
\begin{equation}
\label{eq:wis_intervals}
\WIS(F,Y) = \sum_\alpha \Big\{\alpha(u_\alpha - \ell_\alpha) + 2 \cdot
\dist(Y, [\ell_\alpha, u_\alpha])\Big\},  
\end{equation}
where $\dist(a, S)$ is the distance between a point $a$ and set $S$ (the
smallest distance between $a$ and an element of $S$).  Note that, corresponding 
to \eqref{eq:quantile_levels}, the exclusion probabilities are $\alpha \in
\{0.05, 0.2, 0.5, 1\}$, resulting in 4 terms in the above sum.  By 
straightforward algebra, it is not hard to see WIS has an alternative 
representation in terms of the predicted quantiles themselves:
\begin{equation}
\label{eq:wis_quantiles}
\WIS(F,Y) = 2 \sum_\tau \phi_\tau(Y - q_\tau), 
\end{equation}
where $\phi_\tau(x) = \tau |x|$ for $x \geq 0$ and $\phi_\tau(x) = (1-\tau)
|x|$ for $x<0$, which is often called the ``tilted absolute'' loss.  While
\eqref{eq:wis_quantiles} is more general (it can accomodate asymmetric quantile
levels), the first definition \eqref{eq:wis_intervals} is usually preferred in
presentation, because the score nicely decouples into a ``sharpness'' component
(first term in each summand) and ``under/overprediction'' component (second term
in each summand).  The second form given in \eqref{eq:wis_quantiles} is
especially noteworthy in our current study as it reveals WIS is the same as the
quantile regression loss that we use to train our forecasting models (i.e., our
models are fit to optimize WIS averaged over the training set).    

For hotspot prediction, we evaluate the probabilistic classifiers produced by
the logistic models in \eqref{eq:hotspot_ar}, \eqref{eq:hotspot_ar_x} using the 
area under the curve (AUC) of their true positive versus false positive rate
curve (which is traced out by varying the discrimination threshold). 

The primary aggregation scheme that we will use in model evaluation and
comparisons will be to average WIS per forecaster per ahead value $a$, over all
forecast dates $t$ and locations $\ell$; and similarly, to compute AUC per
classifier and per ahead value $a$, over all forecast dates $t$ and locations
$\ell$.

\subsection{Other Considerations}

\paragraph{Missing Data Imputation}

Over the prediction period, all auxiliary indicators are available (in the
proper vintage sense) for all locations and prediction times, except for the
Google-AA signal, which is only observed for an average of 105 (of 306) HRRs.
This is due to the fact that that the COVID-19 search trends data is released
after applying differential privacy techniques \cite{Bavadekar:2020}, and a
missing signal value means that the level of noise added in the differential
privacy mechanism is high compared to the underyling search count.  In other
words, values of the Google-AA signal are clearly \textit{not} missing at
random.  It seems most appropriate to impute missing values by zero, and this 
is what we do in our analysis.

\paragraph{Backfill and Nowcasting}  

As described previously, the auxiliary indicators defined in terms of medical
claims, CHNG-CLI, CHNG-COVID, and DV-CLI, undergo a significant and systematic
pattern of revision, or ``backfill'', after their initial publication.  Given
their somewhat statistically-regular backfill profiles, it would be reasonable
to attempt to estimate their finalized values based on vintage data---a problem
we refer to as \textit{nowcasting}---as a pre-processing step before using them
as features in the models in \eqref{eq:forecast_ar_x}, \eqref{eq:hotspot_ar_x}. 
Nowcasting is itself a highly nontrivial modeling problem, and we do not attempt
it in this paper (it is a topic of ongoing work in our research group), but we
note that nowcasting would likely improve the performance of the models
involving claims-based signals in particular.        

\paragraph{Spatial Heterogeneity} 

Some signals display a significant degree of spatial heterogeneity, by which we
mean the values that they take at different geographic locations are not 
comparable.  This is true of the Google-AA signal (due to the way in which the 
underlying search trends time series is self-normalized, see
\cite{GoogleSymptoms}) and of the claims-based signals (see the discussion in
our companion paper on the API and database).  Such spatial heterogeneity likely
hurts the performance of the predictive models that rely on these signals,
as we train the models on data pooled over all locations.  In the current 
paper, we do not attempt to address this issue (it is again a topic of ongoing
work in our group), and we simply note that location-specific effects
(or pre-processing to remove spatial bias) would likely improve the performance 
of the models involving Google-AA and the claims-based indicators.

\section{Results}

We begin with a summary of the high-level conclusions.  Here, and in the
following subsections, we will use ``AR'' to refer to the pure autoregressive
model both in forecasting \eqref{eq:forecast_ar} and in hotspot prediction
\eqref{eq:hotspot_ar} (where the reference to the task should be clear from the
context). We will also use the name of an auxiliary indicator---``CHNG-CLI'', 
``CHNG-CLI'', ``CHNG-COVID'', ``CTIS-CLI-in-community'', ``DV-CLI'', or
``Google-AA''---interchangeably with the model in forecasting
\eqref{eq:forecast_ar_x} or hotspot prediction \eqref{eq:hotspot_ar_x} that uses
this particular indicator as a feature (the meaning should be clear from the
context).  So, e.g., the CHNG-CLI model in forecasting is the one in
\eqref{eq:forecast_ar_x} that sets $X_{\ell,t}$ to be the value of the CHNG-CLI
indicator at location $\ell$ and time $t$, fits the coefficients based on the
training data, and makes its predictions by setting $X_{\ell,t}$ to the
appropriate CHNG-CLI values.  Finally, we use the term ``indicator model'' to 
broadly refer to any one of the ten models of the form \eqref{eq:forecast_ar_x},
\eqref{eq:hotspot_ar_x} (five in each of the forecasting and hotspot prediction
tasks).

\begin{itemize}
\item Stratifying predictions by the ahead value ($a=7,\ldots,21$), and
  aggregating results over the prediction period (early June through end of
  December 2020), each of the indicator models generally gives a boost in
  predictive accuracy over the AR model, in both the forecasting and hotspot 
  prediction tasks.  The gains in accuracy generally attenuate as the ahead 
  value grows. 

\item In the same aggregate view, CHNG-COVID and DV-CLI offer the biggest 
  gains in each of forecasting and hotspot prediction.  CHNG-CLI is
  inconsistent: it provides a big gain in hotspot prediction, but little gain 
  in forecasting (it seems to be hurt by a notable lack of robustness, due
  to backfill).  CTIS-CLI-in-community and Google-AA each provide decent 
  gains in forecasting and hotspot prediction.  The former's performance in 
  forecasting is notable in that it clearly improves on AR even at the largest
  ahead values.   

\item In a more detailed analysis of forecasting performance, the indicator
  models tend to be worse than AR when case rates are increasing (this is most
  notable in CHNG-CLI and DV-CLI), and better than AR when case rates are flat
  or decreasing (most notable in CHNG-COV and CTIS-CLI-in-community).
  More rarely does an indicator model tend to beat AR when case rates are
  increasing, but there appears to be some evidence of this for Google-AA. 
  % In an increasing period, when an indicator model is worse than AR, it tends
  % to underpredict relative to AR (its median forecast is lower).  In a flat
  % or decreasing  period, when an indicator model is better than AR, AR tends
  % to overpredict relative to it (the median AR forecast is higher).   

\item In this same analysis, when an indicator model performs better than AR  
  in a decreasing period, this tends to co-occur with instances in which the
  indicator ``leads'' case rates (meaning, roughly, on a short-time scale in a
  given location, its behavior mimics that of case rates some number of days
   ahead).  On the other hand, neither leading nor lagging behavior tend to
  co-occur as consistently with an indicator model doing better in periods of
  increase, or worse in periods of increase or decrease.  
\end{itemize}

Finally, to quantify the importance of training and making predictions using
proper vintage data, we ran a parallel set of forecasting and hotspot prediction 
experiments using finalized data. The results, given in the supplement, show
that training and making predictions on finalized data can result in grossly
optimistic estimates of true test-time performance (up to 10\% better in terms
of average WIS or AUC).

Code to reproduce all results (which uses the \texttt{evalcast} R package) can 
be found at
\url{https://www.github.com/cmu-delphi/covidcast-pnas/forecast/code/}. 

\subsection{Primary Results}

Figure \ref{fig:topline-overall-results} displays the primary results
of this paper, showing the effect of using indicators on the two
forecasting tasks as a function of the horizon of the forecast (i.e.,
the number of days ahead one is forecasting). For the case rate forecasting
task, the left panel shows the ratio of the mean
WIS 
of each forecaster relative to the mean WIS of the
baseline. \attn{Further description and justification of this metric
  should be added to the ``Evaluation Metrics'' section rather than
  here it seems?}  Both means are taken over all HRRs and forecast
dates ranging from June 9, 2020 to December 31, 2020 \attn{verify}.
All curves are well below 1, which means that all methods, including \ar, outperform the baseline across all
considered forecast horizons. The claims-based
indicators \dv~and \chngcov~show the most substantial gains, whereas
the \chngcli~does not show any improvement over \ar.  The \fb~and
\gs~indicators also show a non-negligible improvement.  Notably,
\fb~is the only indicator to offer a clear improvement at far aheads.
%\begin{itemize}
%\item \dv~and \chngcov~perform very well
%\item \fb~and \gs~perform well
%\item \chngcli~performs poorly
%\item Interestingly, fb is the only one offering clear improvement at
 % far aheads
%\item Note the y-axis: consistent, clear improvement over baseline
%\end{itemize}

For the hotspot prediction task, the right panel of Figure
\ref{fig:topline-overall-results} shows the AUC of each method.
The monotonic decrease of all curves reflects the fact that hotspot
prediction gets harder as the forecasting horizon increases. At the
highest considered ahead (nearly one month ahead), the AUCs are close
  to 0.5, which is no better than random guessing.
In the 7-14 day ahead range, the indicators show a noticeable
  improvement over \ar.  For larger horizons there does not
  appear to be much difference in methods.
The approximate ordering of methods is similar to what is seen
  in the forecasting task: \chngcov~and \dv~are the strongest
  performing and \ar~the weakest.
Even for 7-day ahead predictions, the AUC is below 0.7,
  suggesting that all methods struggle with this classification
  task. \attn{Do people agree that an AUC of 0.7 is not all that great?}
  That said, \dv~and \chngcov~attain roughly a 10\% increase in AUC
  over the \ar~classifier.
  
%\begin{itemize}
%\item Monotonic decrease reflects the fact that hotspot
%  prediction gets harder as ahead increases.
%\item At the highest ahead (almost one month ahead) the AUCs are close
%  to 0.5, which is no better than random guessing.
%\item In the 7-14 day ahead range, the indicators show a noticeable
%  improvement over the \ar.  For larger horizons there does not
%  appear to be much difference in methods
%\item The approximate ordering of methods is similar to what is seen
%  in the forecasting task: chng\_cov and dv are the strongest
%  performing and \ar the weakest.
%\item Even for 7-day ahead predictions, the AUC is below 0.7,
%  suggesting that all methods struggle with this classification task.
%  That said, dv and chng\_cov attain roughly a 10\% increase in AUC
%  over the \ar classifier.
%\end{itemize}


% We see that all three \attn{will need to
% include imputed-zero google here, hopefully four} indicator-assisted
% forecasters improve upon the baseline QAR3 forecaster.  The QAR3 model
% assisted by the Change Healthcare confirmed COVID-19 signal
% (QAR3+CHCOV3) sees the strongest improvement.
% C`

\begin{figure}[tb!]
  \centering
  \includegraphics[width=\linewidth]{fig/figure5.pdf}
  \caption{\label{fig:topline-overall-results}  Forecasting performance as a
    function of number of days 
    ahead, aggregated across all HRRs and forecast dates. \attn{How's this?}}
\end{figure}

\subsection{Avoiding misleading retrospective evaluations}

Figure \ref{fig:finalized-vs-honest} quantifies the effect of
not properly accounting for the question of ``what was known when'' in
performing retrospective evaluations of forecasters.  When methods are
given the finalized version of the data rather than the version available at the time that the forecast would have been made, all methods
appear (falsely) to have better performance.  For example, for forecasting case rates
7-days ahead, the WIS of all methods is at least 8\% larger than what would have been
recorded using the finalized values of the data.  This effect
diminishes as the forecasting horizon increases, reflecting the fact
that these forecasters rely less heavily on recent data than very
short-term forecasters.  Crucially, some methods are
``helped'' more than others by the less scrupulous retrospective
evaluation, underscoring the difficulty of avoiding misleading
conclusions when performing retrospective evaluations of forecasters.

The \chngcli~indicator (along with the other claims-based signals) is the most
affected by this distinction, reflecting the latency in claims-based
reporting. \attn{Actually, \dv~is the least affected for forecasting
  whereas it is one of the most affected for hotspots.}  This supports
the importance of efforts to provide ``nowcasts'' for claims signals
(which corresponds to a 0-ahead ``forecast'' of what the claims
signal's value will be once all data has been collected).

Even the \ar~model is affected by this distinction, reflecting the
fact that the case rates themselves (i.e., the response values) are also
subject to revision.  The forecasters based on indicators are thus
affected both by revisions to the indicators and by revisions to the
case rates.

\begin{itemize}
%\item Motivates the importance of nowcasting for claims signals.
  \item For forecasting task: Also, \chngcli~and \dv~perform very similarly here, which is
    reassuring because they're measuring the same thing (in
    principle).  Yet they must have very different backfill profiles…
  \item For hotspots task: dv seems to get a bigger boost than
    \chngcov - interesting dichotomy to forecasting
\item But in both forecasting/hotspots \chngcli~is affected a lot by backfill
\item \attn{We should be careful about which indicators we were not
      able to track latency for.  In fact, we should probably remove
      these from the plot since they would be more strongly affected
      than is shown in the plot.}

\item \attn{We can also discuss the distinction
      between indicators that could in principle have low latency
      (e.g. GS) versus those where latency is inherent
      (e.g. claims-based indicators). If the goal is assessing the
      performance of GS ``for the next pandemic'' then it would make
      sense to give GS a pass since in theory they could have had it
      with very low latency (as opposed to claims based signals where
      backfill may be unavoidable).  Of course, in
    ``the next pandemic'' there might not be such highly specific
    symptoms such as A+A as there was in the COVID-19 case.} 
\end{itemize}

\begin{figure}[tb!]
     \centering
     \includegraphics[width=\linewidth]{fig/figure6.pdf}
     \caption{How misleading is a retrospective analysis that uses the
       finalized version of the data rather than the version that was
       in fact available at the time the forecast was made?  Plots
       show the ratio in performance (using mean WIS fore the
       forecasting task and AUC for the hotspot prediction
       task). \label{fig:finalized-vs-honest} Cheating vs. honest plots.} 
\end{figure}

\subsection{Additional sensitivity analyses}

In addition to whether one properly accounts for signal latency, there
are several other choices in evaluating forecasters that can also have
a non-trivial impact on results.

For example, in Figure
\ref{fig:topline-overall-results}, we show the ratio
of means.  Other choices of aggregation and scaling are also
possible.  For example, taking the mean of ratios of errors is
possible and would give more importance to locations where
errors are small (either because the location-time pairs had low case
rates or because the forecasting task was itself easier).  By
contrast, the ratio of the mean of errors may be dominated by
location-time pairs where the error is large.
Figure \ref{fig:
  scale-aggregate-order} of the Supplementary Information shows...
\begin{itemize}
\item Note the y-axis: barely an improvement over baseline, and this goes away for AR after 11 days ahead, and for all other models around 13-14 days ahead.
\item Using trimmed means here because otherwise the results are extremely unstable (as the denominators can be small, when WIS of baseline is small).
\item Similar message to before (non-adjusted), but \dv~and \chngcov~the best, \fb~and \gs~still good, but now \chngcli~seems to do much better!  
\item What's our interpretation here?  That somehow \chngcli~just
  does particularly bad in tasks where the baseline is also bad?
  \item We might include a histogram of WIS with a log-transformed
    x-axis to suggest that the geometric mean may be reasonable here.
\end{itemize}
Using instead the geometric mean makes the order of aggregation and
scaling immaterial since the ratio of geometric means is the same as
the geometric mean of ratios.  Figure \ref{fig:geometric-mean} of the
Supplementary Information shows how Figure
\ref{fig:topline-overall-results} panel A would change if
one replaces the arithmetic mean by the geometric mean.

In Supplementary Information \ref{sec:2021}, we investigate the effect of lengthening
the time window to include 2021.
\begin{itemize}
\item For forecasting: results get better, whether we look at
  non-adjusted (first plot) or adjusted scores (second plot).  Other
  than that, interpretation is qualitatively similar, except fb now
  seems to be worse than gs.  And \chngcli~is strong, even in the
  non-adjusted view.
  \item For hotspots: Alden will write something explaining why we
    don't do hotspot detection in 2021.
\end{itemize}

In Supplementary Information Section \ref{sec:gs-locations}, we look at various approaches to
accounting for the fact that the \gs~indicator is only available
for XX HRRs.

\begin{itemize}
  \item For forecasting task: clearly gs is not missing at random, and when it's present, it tends to be predictive.  Hence high values have a low false positivity rate.  Most visible in the adjusted view below, where gs triumphs.  Also \chngcli~gets a lot worse.

\item For hotspots: All methods look better, suggesting that the “GS locations” are easier for hotspot prediction. E.g. at 7-days ahead, the AUCs computed based on all locations range from 0.61-0.66; restricted to GS locations, the AUCs range from about 0.65-0.69. (These are all eyeballed, should get actual numbers if we want to say something like this in paper.)
\item gs\_subset appears to be particularly helped by this subsetting
  (which makes sense since on those other locations it was 0-imputed)
  \item Daniel will also try gs\_impute as in forecasting
\end{itemize}

% Figure \ref{fig:forecast-median-relative-wis-faceted} splits the topline
% median relative WIS results over four time periods:  June and July, which
% corresponds to the small, initial surge experienced in the US;
% August and September, when incident cases nationally were somewhat stable;
% October, when a second surge was witnessed; and November and December,
% which saw the largest surges to date.  In essentially all the time periods,
% the indicator-assisted models perform in aggregate as well or better than
% the baseline QAR3 model.  The results in October do not show much
% differentiation between the baseline QAR3 and the indicator-assisted models.
% However, we note that during that time period, even the baseline QAR3
% model is doing very well in terms of relative WIS (especially compared to
% other time periods), so perhaps there is not much ``room for improvement'',
% so to speak.

% \begin{figure*}[tb!]
%      \centering
%      \begin{subfigure}[b]{0.49\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{fig/reduction_mean_scaled_wis_facet_honest.pdf}
%          \caption{Forecasting task: mean WIS relative to mean WIS of baseline}
%          \label{fig:forecast-reduction-mean-scaled-wis-facet}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.49\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{fig/auc_by_forecast_period.pdf}
%          \caption{Hotspot prediction task: AUC}
%          \label{fig:hotspot-auc-faceted}
%      \end{subfigure}
%      \label{fig:topline-faceted-results}
%      \caption{Topline results from June - December 2020, split into
%      four representative time periods.}
% \end{figure*}


\paragraph{\attn{Additional results section material, some for Discussion}}


\begin{figure}
    \centering
    \includegraphics[width=.95\linewidth]{fig/fake-map.pdf}
    \caption{Weighted interval score for forecasts made on April 1, 2021. Scores
      are averaged over horizons from 1 to 14 days ahead and are relative to
      population size.} 
    \label{fig:fake-map}
\end{figure}

\begin{enumerate}
%\item How does this vary over time? Table showing 7-day ahead and 14-day ahead
%  (just picking out two of the d=5,...,20). Include a national curve.
\item Spatial map of errors across HRRs for both tasks. See
  \autoref{fig:fake-map} for an example. 
\item Unlikely we can find it, but we should try anyway. Possibly put in the
  intro or discussion instead. Find an example of a forecast altered by the
  presence of an indicator.  Plot the two trajectories.
\item Examine a specific HRR over time. What does it mean to predict certain
  hotspots? How can we understand what is happening?
 \item What can we say about statistical significance? NRI, paired t-test?
 \item Figures showing hotspots make sense
 \item Horizontal line at 0.5 on AUC curve, discribe that this is random
   guessing. 
\end{enumerate}


\section{Discussion}

We have assessed the performance of [NUMBER] indicators in terms of their
usefulness for performing two forecasting tasks: probabilistic
forecasting of case
rates and hotspot prediction at the HRR-level, 1--[NUMBER] days in advance.
In measuring the practical usefulness of an indicator to forecasting,
we distinguish between honest versus aspirational assessments.  Data
latency is an important factor when considering the usefulness of an
indicator.  Retrospective evaluations that ignore these messy data
issues fail to provide the full picture.  While one can hope that reporting
systems will operate faster and more consistently in ``the next
pandemic,'' some reasons for this latency appear unavoidable.  The
chaos and strain inherent to pandemic times make indicators that can
be reported and collected with quickly with minimal effort or manual
involvement desirable.

We conclude by observing that the
approach we have taken in this paper is somewhat opposite to that of much of the COVID-19
forecasting literature.  We have chosen to consider only very simple forecasting models while devoting most of our effort to accounting for as much of the complexity of
the data and evaluation as possible.  By contrast, many papers focus
on very complicated forecasting approaches but then evaluate them
under unrealistic, retrospective conditions.


\begin{enumerate}

\item Discussion should contain more detailed lit review ("context"), rather
  than in the intro, per the instructions. 

\item Use of syndromic surveillance for other diseases?

\item Cite any papers that use our signals in forecasting.
\end{enumerate}

\paragraph{\attn{Other potential discussion topics}}



\begin{itemize}

\item When was this data actually available? (As of issue. The argument is: sure
  we didn't have it, but it would have helped. We should have this next time.)
\item Side issues, are there places / times where one signal is especially good?
  Especially bad?
\item What do we do about holidays? How do we understand the performance when
  the data is trash? If the data is trash, all the more reason to have other
  data that is perhaps less susceptible to these criticisms.
\item Other signals we might want? Mobility? Discuss how SEIR models need
  contact matrices, relationship with ``in community'' signals.
  
\end{itemize}

\showmatmethods{} % Display the Materials and Methods section

\acknow{Please include your acknowledgments here, set in a single paragraph. Please do not include any acknowledgments in the Supporting Information, or anywhere else in the manuscript.}

\showacknow{} % Display the acknowledgments section

% Bibliography
\bibliography{../../common/covidcast.bib}

\end{document}
