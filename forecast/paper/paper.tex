\documentclass[9pt,twocolumn,twoside,lineno]{pnas-new}
% Use the lineno option to display guide line numbers if required.

\templatetype{pnasresearcharticle} % Choose template 
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} %= Template for a one-column mathematics article
% {pnasinvited} %= Template for a PNAS invited submission

\input{../../common/prelim.tex}

\title{Can Auxiliary Indicators Improve COVID-19 Forecasting and Hotspot  
  Prediction?} 

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[a,c,1]{Author One}
\author[b,1,2]{Author Two} 
\author[a]{Author Three}

\affil[a]{Affiliation One}
\affil[b]{Affiliation Two}
\affil[c]{Affiliation Three}

% Please give the surname of the lead author for the running footer
\leadauthor{Lead author last name} 

% Please add a significance statement to explain the relevance of your work
\significancestatement{
  TODO % For reference, below is the indicators paper one.

  % To study the COVID-19 pandemic, its effects on society,
  % and measures for reducing its spread, researchers need detailed data on the
  % course of the pandemic. Standard public health data streams suffer
  % inconsistent reporting and frequent, unexpected revisions. They also miss
  % other aspects of a population's behavior, worthy of consideration. We
  % present an open database of COVID signals in the United States, measured at
  % the county level and updated daily. These include traditionally reported
  % COVID cases and deaths, and many others: signals based on measures of
  % mobility, social distancing, internet search trends, self-reported symptoms,
  % and patterns COVID-related activity in de-identified medical insurance 
  % claims. The database provides all signals in a common, easy-to-use format,
  % empowering both  public health research and operational decision-making. 
}

% Please include corresponding author, author contribution and author
% declaration information 
\authorcontributions{Please provide details of author contributions here.}
\authordeclaration{Please declare any competing interests here.}
\equalauthors{\textsuperscript{1}A.O.(Author One) contributed equally to this
  work with A.T. (Author Two) (remove if not applicable).} 
\correspondingauthor{\textsuperscript{2}To whom correspondence should be
  addressed. E-mail: author.two\@email.com} 

% At least three keywords are required at submission. Please provide three to
% five keywords, separated by the pipe symbol. 
\keywords{Keyword 1 $|$ Keyword 2 $|$ Keyword 3 $|$ ...} 

\begin{abstract}
Please provide an abstract of no more than 250 words in a single paragraph.
Abstracts should explain to the general reader the major contributions of the
article. References in the abstract must be cited in full within the abstract
itself and cited in the text. 
\end{abstract}

\dates{This manuscript was compiled on \today}
\doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}

\begin{document}

\maketitle
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}

\dropcap{T}racking and forecasting indicators from public health reporting
streams---such as confirmed cases and deaths in the COVID-19 pandemic---is
crucial for understanding disease spread, formulating public policy responses,
and anticipating future public health resource needs.  In a companion paper, we
describe our research group's (Delphi's) efforts in curating and maintaining a 
database of real-time indicators that track COVID-19 activity and other relevant
phenomena. The signals (a term we use synonomously with ``indicators'') in this
database are accessible through the COVIDcast API \cite{CovidcastAPI}, with
associated R \cite{CovidcastR} and Python \cite{CovidcastPy} packages for
convenient data fetching and processing tools. In the current paper, we aim to
quantify the utility provided by a core set of these indicators for two
fundamental prediction tasks: probabilistic forecasting of COVID-19 case 
rates and prediction of future COVID-19 case hotspots (defined by the event that
a relative increase in COVID-19 cases exceeds a certain threshold). 

At the outset, we should be clear that our intent in this paper is \textit{not}
to provide an authoritative take on cutting-edge COVID-19 forecasting methods.
Instead, our purpose here is to provide a rigorous, quantitative assessment of
the utility that several auxiliary indicators---such as those derived from
internet surveys or medical insurance claims---provide in tasks that involve
predicting future trends in confirmed COVID-19 cases. To assess such utility in
as simple terms as possible, we center our study in the framework of a basic
autoregressive model (in which COVID cases in the near future are predicted from  
a linear combination of COVID cases in the near past), and ask whether the 
inclusion of an auxiliary indicator as an additional feature in such a model
improves its predictions. 

While forecasting carries a rich literature that offers a wide range of
techniques, see e.g., \cite{Hyndman:2018}, we purposely constrain ourselves to
very simple models, avoiding common enhancements such as order selection,
correction of outliers/anomalies in the data, and inclusion of regularization or
nonlinearities. That said, analyses of forecasts submitted to the COVID-19
Forecast Hub \cite{ForecastHub} by a large community of modelers have shown that
simple, robust models have consistently been among the best-performing over the
pandemic \cite{Cramer:2021}, including time series models similar to those we
consider in what follows.   

In our companion paper, we analyze correlations between various indicators and   
COVID case rates. These correlations are natural summaries of the
contemporaneous association between an indicator and COVID cases, but they fall 
short of delivering a satisfactory answer to the question that motivates the
current article: is the information contained in an indicator demonstrably
useful for the prediction tasks we care about? For such a question, lagged
correlations (e.g., measuring the correlation between an indicator and COVID
case rates several days in the future) move us in the right direction, but still
do not move us all the way there. The question about \textit{utility for 
  prediction} is focused on a much higher standard than simply asking 
about correlations; to be useful in forecast or hotspot models, an indicator
must provide relevant information that is not otherwise contained in past values
of the case rate series itself. We will assess this in the most direct way
possible: by inspecting the difference in predictive performance of simple
autoregressive models trained with and without access to past values of a
particular indicator.   

There is another, more subtle issue in evaluating predictive utility that 
deserves explicit mention, as it will play a key role in our analysis.
Signals computed from surveillance streams will often be subject to  
latency and/or revision. For example, a signal based on aggregated medical
insurance claims may be available after just a few days, but it can then be
substantially revised over the next several weeks as additional claims are
submitted and/or processed late. Correlations between such a signal and case
rates calculated ``after the fact'' (i.e., computed retrospectively, using the
finalized values of this signal) will not deliver an honest answer to the    
question of whether this signal would have been useful in real time. Instead,
we build predictive models using only the data that would have been available
\textit{as of} the prediction date, and compare the ensuing predictions in terms 
of accuracy. To do so, we leverage Delphi's \texttt{evalcast} R package 
\cite{EvalcastR}, which plugs into the COVIDcast API's data versioning system,  
and facilitates honest backtesting. 

Finally, it is worth noting that examining the importance of additional features
for prediction is a core question in inferential statistics and econometrics,
with work dating back to at least \cite{Granger:1969}. Still today, drawing
rigorous inference based on predictions, without (or with lean) assumptions, is
an active field of research from both the applied and theoretical angles;
see, e.g., \cite{Diebold:2002, McCraken:2007, Diebold:2015, Stokes:2017,
  Lei:2018, Rinaldo:2019, Williamsom:2020, Zhang:2020, Dai:2021, Fryer:2021}.
Our take on this problem is in line with much of this literature; however, in
order to avoid making any explicit assumptions, we do not attempt to make formal 
significance statements, and instead, broadly examine the stability of our
conclusions with respect to numerous modes of analysis.   

\section{Methods}

\subsection{Signals and Locations}
\label{sec:signals-locations}

We consider prediction of future COVID-19 case rates or case hotspots (to be
defined precisely shortly).  By case rate, we mean the case count per 100,000
people (the standard in epidemiology).  We use reported case data aggregated by 
JHU CSSE \cite{Dong:2020}, which is accessible through the COVIDcast API
\cite{CovidcastAPI}, like the auxiliary indicators that we use to supplement the
basic purely autoregressive models.   

The indicators we focus on provide information not generally available from
standard public health reporting. Among the many auxiliary indicators collected
in the API, we study the following five:  
\begin{itemize}
\item Change Healthcare COVID-like illness (CHNG-CLI): The percentage of
  outpatient visits that are primarily about COVID-related symptoms, based on
  de-identified Change Healthcare claims data.
\item Change Healthcare COVID (CHNG-COVID): The percentage of outpatient visits
  with confirmed COVID-19, based on the same claims data.
\item COVID Trends and Impact Survey CLI in the community
  (CTIS-CLI-in-community): The estimated percentage of the population who know
  someone in their local community that is sick, based on Delphi's surveys of
  Facebook users, called the COVID Trends and Impact Survey. 
\item Doctor Visits COVID-like illness (DV-CLI): The same as CHNG-CLI, but
  computed based on de-identified medical insurance claims from other health
  systems partners.  
\item Google Search Trends for anosmia and ageusia (Google-AA): A measure of 
  COVID-19 related Google search volume for queries relating to anosmia or
  ageusia (loss of smell or taste), based on Google's Search Trends data set. 
\end{itemize}
In short, we choose these indicators because, conceptually speaking, they
measure aspects of an individual's disease progression that would plausibly
precede the occurence of (at worst, co-occur  with) the report of a positive
COVID-19 test, through standard public health reporting streams.

For more details on the five indicators (including how these are precisely
computed from the underlying data streams) we refer to
\url{https://cmu-delphi.github.io/delphi-epidata/api/covidcast_signals.html},
which documents all of the signals in the COVIDcast API, and our companion paper
on the API and database. For CTIS in particular, we refer to our companion paper
on this survey. For the Google Search Trends data set, see
\cite{GoogleSymptoms}, and \cite{Klopfen:2020, Vaira:2020} for articles on the
relevance of anosmia or ageusia (loss of smell or taste) to COVID.

As for geographic resolution, we consider the prediction of COVID-19 case rates
and hotspots aggregated at the level of an individual \textit{hospital referral
  region} (HRR). HRRs correspond to groups of counties in the United States
within the same hospital referral system. The Dartmouth Atlas of Healthcare 
Policy~\cite{DartmouthHRR}, defines these 306 regions based on a number of
characteristics. They are contiguous regions such that most of the hospital
services for the underlying population are performed by hospitals within the
region. Each HRR also contains at least one city where major procedures
(cardiovascular or neurological) are performed. The smallest HRR has a
population of about 120,000. While some are quite large (such as the one
containing Los Angeles, which has about 10 million people), generally HRRs 
are much more homogenous in size than the (approximately) 3200 counties,  
and they serve as a nice middle ground in between counties and states.  

HRRs, by their definition, would be most relevant for forecasting hospital
demand.  We have chosen to focus on cases (forecasting and predicting
hotspots) at the HRR level since the indicators considered should be more 
useful in predicting case activity versus hospital demand, since the latter is
intuitively more closely connected (in time) to the events that are measured by
the given five indicators. Predicting case rates (and hotspots) at the HRR level
is still a reasonable goal in its own right; and moreover, it could be used to
feed predicted case information into downstream hospitalization models.

\subsection{Vintage Training Data}

In this paper, all models are fit with ``vintage'' training data. This means
that for a given prediction date, say, September 1, 2020, we train models 
using only the data that would have been available to us \textit{as of}
September 1 (imagine that we can ``rewind'' the clock to September 1, and query
the COVIDcast API to get the latest data it would have had available at that
point in time.)  This is made possible by the COVIDcast API's comprehensive data   
versioning system (described in more detail in our companion paper).  We also
use the \texttt{evalcast} R package \cite{EvalcastR}, which streamlines the
process of training arbitrary prediction models over a sequence of prediction
dates, by constructing the proper sequence of vintage training data sets.   

\begin{figure}[tb!]
  \includegraphics[width=\columnwidth]{fig/revisions.pdf}
  \caption{Revision behavior for two signals in the HRR containing New York City
    HRR.  Each colored line corresponds to the data as reported on a particular
    date (\textit{as of} dates varying from September 28 through October
    19). The left panel shows the DV-CLI signal, which was regularly revised
    throughout the period, though the effects fade as we look further back in   
    time. In contrast, the right panel shows case rates reported by JHU CSSE,
    which remain ``as reported'' on September 28, with a huge spike towards the 
    end of this period, until a major correction is made on October 19, which
    brings down this spike and affects all prior data as well.}
  \label{fig:vintage}
\end{figure}

Vintage training data means different things, in practice, for different
signals. The three signals based on medical claims, CHNG-CLI, CHNG-COVID, and
DV-CLI, are most often 3-5 days latent, and subject to a considerable but
regular degree of revision or ``backfill'' after their initial publication date.
The survey-based signal, CTIS-CLI-in-community, is 2 days latent, and rarely
undergoes any revision at all.  The target variable itself, reported COVID-19
case rates, is 1 day latent, and exhibits frequent, unpredictable revisions
(sometimes clear anomalies) after initial publication.  Compared to the revision
pattern in the medical claims signals, which are much more systematic in nature,
revisions in case reports can be highly erratic. Big spikes or other anomalies
can occur in the data as reporting backlogs are cleared, changes in case
definitions are made, etc. Groups like JHU CSSE then work tirelessly to correct
such anomalies after first publication (e.g., they will attempt to
back-distribute a spike when a reporting backlog is cleared, by working with a
local authority to figure out how this should best be done), which can result
in very nontrivial and often unpredictable revisions.  See
Figure~\ref{fig:vintage} for an example. 

Lastly, our treatment of the Google-AA signal is different from the rest.
Because Google's team did not start publishing this signal until early
September, 2020, we do not have true vintage data before then.  Furthermore, the
latency of the signal was always at least one week through 2020.  However, this
signal is never revised after initial publication (confirmed via personal
communication with the Google team that produces this signal) and furthermore
the latency of the signal is not an unavoidable property of the data type, so we
simply use finalized signal values it, with zero latency, in our analysis.   

\subsection{Analysis Tasks}

\begin{table*}[t]
\centering
\caption{Summary of forecasting and hotspot prediction tasks considered in
  this paper.}
\begin{tabular}{l p{2.75in} p{2.75in}}
  \toprule
  & \textbf{Forecasting} & \textbf{Hotspot prediction} \\
  \midrule
  Response variable & $Y_{\ell,t}$ (7-day trailing average of COVID-19 case 
incidence rates, per location $\ell$ and time $t$) & $Z_{\ell,t} =
\indicator{Y_{\ell,t} \geq 1.25 \cdot Y_{\ell,t-7}}$ (indicator that 
$Y_{\ell,t}$ grows by more than 25\% relative to the preceding week) \\ 
  Geographic resolution & Hospital referral region (HRR) & Hospital referral
region (HRR) \\ 
  Forecast period & June 9--December 31, 2020 & June 16--December 31, 2020 \\  
  Model type & Quantile regression & Logistic regression \\
  Evaluation metric & Weighted interval score (WIS) & Area under curve (AUC) \\
  \bottomrule
\end{tabular}
\label{tab:analysis_tasks}
\end{table*}

To fix notation, let $Y_{\ell,t}$ denote the 7-day trailing average of COVID-19
case incidence rates in location (HRR) $\ell$ and at time (day) $t$.  To be
clear, this is the number of new daily reported cases per 100,000
people, averaged over the 7-day period $t-6, \ldots t$. The first task we
consider---\textit{forecasting}---is to predict $Y_{\ell,t+a}$ for each
``ahead'' value $a=7,\ldots,21$.  The second task---\textit{hotspot
  prediction}---is to predict a binary variable defined in terms of the relative
change of $Y_{\ell,t+a}$ (relative to its value one week prior,
$Y_{\ell,t+a-7}$), again for each $a=7,\ldots,21$.   

Why do we define the response variables via 7-day averaging? The short answer 
is robustness: averaging stabilizes the case time series, and accounts for 
uninteresting artifacts like weekend-weekday differences in the series.  Note
that we can of course equivalently view this (equivalent up to a constant
factor) as predicting the HRR-level COVID-19 case incidence rate \textit{summed}
over some 7-day period in the future, and predicting a binary variable derived
from this.  

In what follows, we cover more details on our two analysis
tasks. Table~\ref{tab:analysis_tasks} presents a summary.   

\paragraph{Dynamic Re-Training}

For each prediction date $t$, we use a 21-day trailing window of data to train
our forecast or hotspot prediction models (so, e.g., the trained models will
differ from those at prediction date $t-1$).  This is done to account for
(potential) nonstationarity.  For simplicity, the forecasting and hotspot
prediction models are always trained on data across all HRRs (i.e., the
coefficients in the models do not account for location-specific effects).   

\paragraph{Prediction Period}

In our analysis, we let the prediction date $t$ run over each day in between
early/mid June and December 31, 2020.  The precise start date differs for 
forecasting and hotspots prediction; for each task it was chosen to be the
earliest date at which the data needed to train all models was available, which
ends up being (per our setup, with 21 days of training data, and lagged values
of signals for features as we will detail shortly) June 9, 2020 for forecasting,
and June 16, 2020 for hotspot prediction. (The bottleneck here is the
CTIS-CLI-in-community signal, which does not exist before early April 2020, when
the survey was first launched). The end date was chosen again with a
consideration to align both tasks as best as possible, and because few hotspots
exist post December 31, 2020, due to the general and gradual decline of the
pandemic in 2021.

\paragraph{Forecasting Models}

Recall $Y_{\ell,t}$ denotes the 7-day trailing average of COVID-19 case
incidence rates in location $\ell$ and at time $t$.  Separately for each
$a=7,\ldots,21$, to predict $Y_{\ell,t+a}$ for ahead value $a$, we consider a
simple probabilistic forecasting model of the form:     
\begin{equation}
\label{eq:forecast_ar}
\Quantile_\tau(Y_{\ell,t+a} \,|\, Y_{\ell,s}, s \leq t)  
= \alpha^{a,\tau} + \sum_{j=0}^2 \beta^{a,\tau}_j Y_{\ell,t-7j}.  
\end{equation}
This model uses current case rates, and the case rates 7 and 14 days ago, in
order to predict (the quantiles of) case rates in the future.  We consider a
total of 7 quantile levels (chosen in accordance with the county-level quantile
levels suggested by the COVID-19 Forecast Hub),      
\begin{equation}
\label{eq:quantile_levels}
\tau \in \{0.025, 0.1, 0.25, 0.5, 0.75, 0.9, 0.975 \}.
\end{equation}
We fit \eqref{eq:forecast_ar} using \textit{quantile regression}
\cite{Koenker:1978, Koenker:2005, Koenker:2006} separately for each $\tau$, 
using data from all 306 HRRs, and within each HRR, using the most recent 21 days
of training data.  This gives us 6,426 training samples for each quantile 
regression problem.    

In addition to this pure autoregressive model, we also consider five
probabilistic forecasting models of the form:  
\begin{multline}
\label{eq:forecast_ar_x}
\Quantile_\tau(Y_{\ell,t+a} \,|\, Y_{\ell,s}, X_{\ell,s}, s \leq t)  
= \\ \alpha^{a,\tau} + \sum_{j=0}^2 \beta^{a,\tau}_j Y_{\ell,t-7j} + 
\sum_{j=0}^2 \gamma^{a,\tau}_j X_{\ell,t-7j},
\end{multline}
where $X_{\ell,t}$ denotes any one of the five auxiliary indicators---CHNG-CLI, 
CHNG-COVID, CTIS-CLI-in-community, DV-CLI, or Google-AA---at location $\ell$ and
time $t$. Note that we apply the same lags (current value, along with the values
7 and 14 days ago) for the auxiliary indicators as we do for the case
rates. Training then proceeds just as before: we use the same 7 quantile levels
in \eqref{eq:quantile_levels}, and fit quantile regression separately for each 
level $\tau$, using data from all 306 HRRs and a trailing window of 21 days of
training data.

At prediction time, in order to deal with possible crossing violations (a
predicted quantile at level $\tau$ exceeds a predicted quantile at level $\tau'
> \tau$), we apply a simple post-hoc sorting.  See Figure~\ref{fig:trajectory}
for an example forecast. 

\begin{figure}[t]
\includegraphics[width=\columnwidth]{fig/ny-trajectory-1.pdf}
\caption{Forecast from the autoregressive model for the New York City HRR made
  on October 15 (vertical line).  The fan displays 50\%, 80\% and 95\% intervals
  while the orange curve shows the median forecast. The connected black points
  represent ``finalized'' data, as reported in May 2021.}
\label{fig:trajectory}
\end{figure}

\paragraph{Hotspot Prediction Models}

Define the binary indicator:
$$
Z_{\ell,t} = \indicator{Y^\Delta_{\ell,t} \geq 0.25},
$$
where we use the notation \smash{$Y^\Delta_{\ell,t} = (Y_{\ell,t} - Y_{\ell,
    t-7})/(Y_{\ell,t-7})$}. In other words, $Z_{\ell,t}=1$ if the number of
newly reported cases over the past 7 days has increased by at least 25\%
compared to the preceding week.  When this occurs, we say location $\ell$ is a
\textit{hotspot} at time $t$.  Empirically, this rule labels about 27\% of
location-time pairs as hotspots, during the prediction period (June 16--December
31, 2020).  

We treat hotspot prediction as a binary classification problem and use a setup 
altogether quite similar to the forecasting setup described previously.
Separately for each $a=7,\ldots,21$, to predict $Z_{\ell,t+a}$, we consider a
simple logistic model:
\begin{equation}
\label{eq:hotspot_ar}
\logit \big( \P(Z_{\ell,t+a} = 1 \,|\, Y_{\ell,s}, s \leq t) \big) 
= \alpha^{a,\tau} + \sum_{j=0}^2 \beta^{a,\tau}_j Y^\Delta_{\ell,t-7j},
\end{equation}
where $\logit(p) = \log(p/(1-p))$, the log-odds of $p$.

In addition to this pure autoregressive model, we also consider five
logistic models of the form:  
\begin{multline}
\label{eq:hotspot_ar_x}
\logit \big( \P(Z_{\ell,t+a} = 1 \,|\, Y_{\ell,s}, X_{\ell,s}, s \leq t) \big)
= \\ \alpha^{a,\tau} + \sum_{j=0}^2 \beta^{a,\tau}_j Y^\Delta_{\ell,t-7j} +  
\sum_{j=0}^2 \gamma^{a,\tau}_j X^\Delta_{\ell,t-7j},
\end{multline}
where we use \smash{$X^\Delta_{\ell,t} = (X_{\ell,t} - X_{\ell,
    t-7})/(X_{\ell,t-7})$}, and again $X_{\ell,t}$ stands for any of the five  
auxiliary indicators at location $\ell$ and time $t$.  We fit all models
\eqref{eq:hotspot_ar}, \eqref{eq:hotspot_ar_x} using logistic regression, 
pooling all 306 HRRs and using a 21-day trailing window for the training data.   

An important detail is that in hotspot prediction we remove all data from
training and evaluation where, on average, fewer than 30 cases (this refers to a
count, not a rate) are observed over the preceding 7 days. This avoids having to
make arbitrary calls for a hotspot (or lack thereof) based on small counts.   

\subsection{Evaluation Metrics}

For forecasting, we evaluate the probabilistic forecasts produced by the
quantile models in \eqref{eq:forecast_ar}, \eqref{eq:forecast_ar_x} using 
\textit{weighted interval score} (WIS), a quantile-based scoring rule; see e.g.,
\cite{Gneiting:2007}.  WIS is a proper score, meaning that its expectation
is minimized by the population quantiles of the target variable.  The use of WIS
in COVID-19 forecast scoring is discussed in \cite{Bracher:2021}; WIS is also
the main evaluation metric used in the COVID-19 Forecast Hub.   

WIS is typically defined for quantile-based forecasts where the quantile levels
are symmetric around 0.5.  This is the case for our choice in
\eqref{eq:quantile_levels}.  Let $F$ be a forecaster comprised of predicted
quantiles $q_\tau$ parametrized by a quantile level $\tau$.  In the case of
symmetric quantile levels, this is equivalent to a collection of central
prediction intervals $(\ell_\alpha, u_\alpha)$, parametrized by an exclusion
probability $\alpha$. The WIS of the forecaster $F$, evaluated at the target
variable $Y$, is defined by:
\begin{equation}
\label{eq:wis_intervals}
\WIS(F,Y) = \sum_\alpha \Big\{\alpha(u_\alpha - \ell_\alpha) + 2 \cdot
\dist(Y, [\ell_\alpha, u_\alpha])\Big\},  
\end{equation}
where $\dist(a, S)$ is the distance between a point $a$ and set $S$ (the
smallest distance between $a$ and an element of $S$).  Note that, corresponding 
to \eqref{eq:quantile_levels}, the exclusion probabilities are $\alpha \in
\{0.05, 0.2, 0.5, 1\}$, resulting in 4 terms in the above sum.  By 
straightforward algebra, it is not hard to see WIS has an alternative 
representation in terms of the predicted quantiles themselves:
\begin{equation}
\label{eq:wis_quantiles}
\WIS(F,Y) = 2 \sum_\tau \phi_\tau(Y - q_\tau), 
\end{equation}
where $\phi_\tau(x) = \tau |x|$ for $x \geq 0$ and $\phi_\tau(x) = (1-\tau)
|x|$ for $x<0$, which is often called the ``tilted absolute'' loss.  While
\eqref{eq:wis_quantiles} is more general (it can accomodate asymmetric quantile
levels), the first definition \eqref{eq:wis_intervals} is usually preferred in
presentation, because the score nicely decouples into a ``sharpness'' component
(first term in each summand) and ``under/overprediction'' component (second term
in each summand).  The second form given in \eqref{eq:wis_quantiles} is
especially noteworthy in our current study as it reveals WIS is the same as the
quantile regression loss that we use to train our forecasting models (i.e., our
models are fit to optimize WIS averaged over the training set).    

For hotspot prediction, we evaluate the probabilistic classifiers produced by
the logistic models in \eqref{eq:hotspot_ar}, \eqref{eq:hotspot_ar_x} using the 
area under the curve (AUC) of their true positive versus false positive rate
curve (which is traced out by varying the discrimination threshold). 

The primary aggregation scheme that we will use in model evaluation and
comparisons will be to average WIS per forecaster per ahead value $a$, over all
forecast dates $t$ and locations $\ell$; and similarly, to compute AUC per
classifier and per ahead value $a$, over all forecast dates $t$ and locations
$\ell$.

\subsection{Other Considerations}

\paragraph{Missing Data Imputation}

Over the prediction period, all auxiliary indicators are available (in the
proper vintage sense) for all locations and prediction times, except for the
Google-AA signal, which is only observed for an average of 105 (of 306) HRRs.
Such missingness occurs because the COVID-19 search trends data is
constructed using differential privacy methods \cite{Bavadekar:2020}, and a 
missing signal value means that the level of noise added in the differential
privacy mechanism is high compared to the underyling search count.  In other
words, values of the Google-AA signal are clearly \textit{not} missing at
random.  It seems most appropriate to impute missing values by zero, and this 
is what we do in our analysis.

\paragraph{Backfill and Nowcasting}  

As described previously, the auxiliary indicators defined in terms of medical
claims, CHNG-CLI, CHNG-COVID, and DV-CLI, undergo a significant and systematic
pattern of revision, or ``backfill'', after their initial publication.  Given
their somewhat statistically-regular backfill profiles, it would be reasonable
to attempt to estimate their finalized values based on vintage data---a problem
we refer to as \textit{nowcasting}---as a pre-processing step before using them
as features in the models in \eqref{eq:forecast_ar_x}, \eqref{eq:hotspot_ar_x}. 
Nowcasting is itself a highly nontrivial modeling problem, and we do not attempt
it in this paper (it is a topic of ongoing work in our research group), but we
note that nowcasting would likely improve the performance of the models
involving claims-based signals in particular.        

\paragraph{Spatial Heterogeneity} 

Some signals have a significant amount of spatial heterogeneity, by which we
mean their values across different geographic locations are not comparable.
This is the case for the Google-AA signal (due to the way in which the
underlying search trends time series is self-normalized, see
\cite{GoogleSymptoms}) and the claims-based signals (see the discussion in our
companion paper on the API and database).  Such spatial heterogeneity likely
hurts the performance of the predictive models that rely on these signals,
because we train the models on data pooled over all locations.  In the current
paper, we do not attempt to address this issue (it is again a topic of ongoing
work in our group), and we simply note that location-specific effects (or
pre-processing to remove spatial bias) would likely improve the performance of
the models involving Google-AA and the claims-based indicators.

\section{Results}

We begin with a summary of the high-level conclusions.  Here, and in the
following subsections, we will use ``AR'' to refer to the pure autoregressive
model both in forecasting \eqref{eq:forecast_ar} and in hotspot prediction
\eqref{eq:hotspot_ar} (where the reference to the task should be clear from the
context). We will also use the name of an auxiliary indicator---``CHNG-CLI'', 
``CHNG-CLI'', ``CHNG-COVID'', ``CTIS-CLI-in-community'', ``DV-CLI'', or
``Google-AA''---interchangeably with the model in forecasting
\eqref{eq:forecast_ar_x} or hotspot prediction \eqref{eq:hotspot_ar_x} that uses
this particular indicator as a feature (the meaning should be clear from the
context).  So, e.g., the CHNG-CLI model in forecasting is the one in
\eqref{eq:forecast_ar_x} that sets $X_{\ell,t}$ to be the value of the CHNG-CLI
indicator at location $\ell$ and time $t$, fits the coefficients based on the
training data, and makes its predictions by setting $X_{\ell,t}$ to the
appropriate CHNG-CLI values.  Finally, we use the term ``indicator model'' to 
refer to any one of the ten models of the form \eqref{eq:forecast_ar_x},
\eqref{eq:hotspot_ar_x} (five from each of the forecasting and hotspot
prediction tasks).

\begin{itemize}
\item Stratifying predictions by the ahead value ($a=7,\ldots,21$), and
  aggregating results over the prediction period (early June through end of
  December 2020), we find that each of the indicator models generally gives a
  boost in predictive accuracy over the AR model, in both the forecasting and
  hotspot prediction tasks.  The gains in accuracy generally attenuate as the
  ahead value grows. 

\item In the same aggregate view, CHNG-COVID and DV-CLI offer the biggest 
  gains in each of forecasting and hotspot prediction.  CHNG-CLI is
  inconsistent: it provides a big gain in hotspot prediction, but little gain 
  in forecasting (it seems to be hurt by a notable lack of robustness, due
  to backfill).  CTIS-CLI-in-community and Google-AA each provide decent 
  gains in forecasting and hotspot prediction.  The former's performance in 
  forecasting is notable in that it clearly improves on AR even at the largest
  ahead values.   

\item In a more detailed analysis of forecasting performance, we find that the
  indicator models tend to be worse than AR when case rates are increasing (this
  is most notable in CHNG-CLI and DV-CLI), and better than AR when case rates
  are flat or decreasing (most notable in CHNG-COVID and CTIS-CLI-in-community).
  More rarely does an indicator model tend to beat AR when case rates are 
  increasing, but there appears to be some evidence of this for the Google-AA
  model. 
  % In an increasing period, when an indicator model is worse than AR, it tends
  % to underpredict relative to AR (its median forecast is lower).  In a flat
  % or decreasing period, when an indicator model is better than AR, AR tends
  % to overpredict relative to it (the median AR forecast is higher).   

\item In this same analysis, when an indicator model performs better than AR in
  a decreasing period, this tends to co-occur with instances in which the
  indicator ``leads'' case rates (meaning, roughly, on a short-time scale in a
  given location, its behavior mimics that of case rates some number of days
  ahead).  On the other hand, neither leading nor lagging behavior tend to
  co-occur as consistently with an indicator model doing better in periods of
  increase, or worse in periods of increase or decrease.   
\end{itemize}

Finally, to quantify the importance of training and making predictions using
proper vintage data, we ran a parallel set of forecasting and hotspot prediction 
experiments using finalized data. The results, given in the supplement, show
that training and making predictions on finalized data can result in overly
optimistic estimates of true test-time performance (up to 10\% better in terms
of average WIS or AUC). Furthermore, since indicators can have greatly different
backfill profiles, the use of finalized data in retrospective evaluations can
even change the relative ordering of models.  For example, CHNG-CLI and DV-CLI
when trained on finalized data perform very similarly for the forecasting task.
This makes sense since they are both claims-based indicators measuring
supposedly the same thing.  However, DV-CLI outperforms CHNG-CLI on vintage
data, reflecting its has a less severe backfill profile.
% TODO note what we said here about the supplement

Code to reproduce all results (which uses the \texttt{evalcast} R package) can 
be found at
\url{https://github.com/cmu-delphi/covidcast-pnas/tree/main/forecast/code}. 

\subsection{Aggregate Results by Ahead Value}

Figure~\ref{fig:forecast} displays the results for forecasting, stratified by
ahead value and averaged over all HRRs and forecast dates (June 9--December 31,
2020).  Shown is the average WIS for each forecast model divided by the averaged
WIS from a baseline model, which is basically a flat-line forecaster (its
median forecast for $Y_{\ell,t+a}$ is always $Y_{\ell,t}$, with predicted
quantiles defined around this flat line based on historical variation). This is
the same baseline model as that used in the COVID-19 Forecast Hub.  Here, we  
divide by the average WIS of the baseline model in order to put the y-axis on an 
interpretable scale.  We can see that all curves are well below 1, which means
(smaller WIS is better) that all of the models, including AR, outperform the
baseline on average over the forecast period (though this advantage narrows as
the ahead value grows).      

We can also see from the figure that CHNG-COVID and DV-CLI offer the biggest
gains over AR at small ahead values, followed by CTIS-CLI-in-community and
Google-AA, with the former providing the biggest gains at large ahead values.
The CHNG-CLI model performs basically the same as AR.  This is likely due to the
fact that CHNG-CLI suffers from some sort of volatility in forecasting due to
backfill.  The evidence for this is twofold: the CHNG-CLI model benefits from a
more robust method of aggregating WIS (geometric mean; shown in the supplement),
and when we train and make predictions on all-finalized data, it provides a big
gain over AR, on par with the best-performing models (also shown in the
supplement).
% TODO note what we said here about the supplement

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{fig/fcast-1.pdf}
  \caption{Average WIS for each forecast model divided by the averaged WIS of
    the flat-line forecaster.}
  \label{fig:forecast}
\end{figure}

Figure~\ref{fig:hotspot} displays the results for hotspot prediction, again
stratified by ahead value and averaged over all HRRs and prediction dates (June 
16--December 31, 2020).  We can see many similarities to the forecasting
results (recall larger AUC is better): CHNG-COVID and DV-CLI offer the biggest
improvement over AR, all models including AR degrade in performance towards the
baseline (this is now just a classifier based on random guessing, which achieves
an AUC of 0.5) as the ahead values grow.  However, one clear difference is that
the CHNG-CLI model performs quite well in hotspot prediction, close to the
best-performing indicator models for many of the ahead values. 

[TODO: should we
insert one line explaining this---you're somewhat protected form volatiliy in a
logistic model thanks to 
the sigmoid function used to define the probabilities? DJM: I think not, seems a
bit far afield.]  

\subsection{Implicit Regularization Hypothesis}

One might ask if the benefits we observe in forecasting and hotspot
prediction have
anything to do with the actual indicator themselves; could it instead be that
the indicators are simply providing \textit{implicit regularization} on top of 
the basic AR model, when we include them as lagged features in
\eqref{eq:forecast_ar_x}, \eqref{eq:hotspot_ar_x}?  

To test this implicit regularization hypothesis, we reran all of the prediction
experiments but with $X_{\ell,t}$ in each indicator model \eqref{eq:forecast_ar_x},
\eqref{eq:hotspot_ar_x} replaced with suitable random noise (bootstrap
samples form an indicator's historical distribution).  The results, shown and
explained more precisely in the supplement, are vastly different (worse) than
the original set of results.  In both forecasting and hotspot prediction, the
``fake'' indicator models (with random noise as auxiliary features) offered
essentially no improvement over the pure AR model, which provides a strong
rejection (informally speaking) of the implicit regularization hypothesis.     

\begin{figure}[t]
\includegraphics[width=\columnwidth]{fig/hot-1.pdf}
\caption{Area under the curve for each forecast model averaged over all
  prediction dates and HRRS.}
\label{fig:hotspot}
\end{figure}

\subsection{Performance in Up, Down, and Flat Periods}

The course of the pandemic has played out very differently across space and
time. Aggregating case rates nationally shows three pronounced waves, but the
behavior is more nuanced at the HRR level. 
Figure \ref{fig:trajectory} is a single example of what a forecast looks like
during a period of relatively flat cases as New York City begins what will
become its second wave. The AR forecast's 50\% confidence interval contains this
upswing, but the median forecast is clearly below the finalized data.
Unfortunately, this behavior is typical of all forecasters: during upswings, the
median forecast tends to fall below the target while the reverse is true during
downswings.

Figure \ref{fig:up-down-flat} shows histograms of the difference between the WIS for the
forecasters with the benefit of the indicators and the WIS of the AR model, but
it stratifies these differences by
whether the target occurs during a period of increasing cases rates (up),
decreasing case rates (down), or flat. To define the increasing period, we use
the same definition we used for the hotspot task in Table
\ref{tab:analysis_tasks}. Thus all hotspots are ``up'' while all non-hotspots
are either flat or decreasing. For the ``down'' scenario, we simply use the opposite of
the hotspot definition: $Y_{\ell,t}$ decreases by more than 25\% relative to the
preceding week.

While the performance of all forecasters degrades during up periods (WIS is on the
scale of cases, so higher case counts means higher WIS) there are pronounced
patterns for the different models. 
Google-AA is the only model that
outperforms the AR model, on average, during upswings (see the supplement for
these averaged conclusions). Both Google-AA and
CTIS-CLI-in-community have large right tails during the flat periods. CHNG-CLI
and DV-CLI have large left tails (indicating some large, relatively poor
forecasts) during flat and up periods. CHNG-CLI, CHNG-COVID, Google-AA, and
especially CTIS-CLI-in-community have large positive outliers during the down
period. Overall, the indicators seem to help more during periods of flat or down
cases than during periods of increasing case rates, with the exception of
Google-AA.
In the supplement, we examine classification accuracy and log-likelihood for the
hotspot task. We see a similar phenomenon: the indicators dramatically improve
hotspot prediction accuracy during flat and down periods with more mixed
behavior during periods of increasing cases, with CHNG-CLI, CHNG-COVID, and DV-CLI
leading to decreased performance. 



\begin{figure}[t]
  \includegraphics[width=\columnwidth]{fig/upswing-histogram-1.pdf}
  \caption{Histogram of the difference in WIS for the AR model relative to each
    forecaster. Larger values are better for the indicator assisted model. The
    $y$-axis is on the logarithmic scale to 
    emphasize outliers.}
  \label{fig:up-down-flat}
\end{figure}

The supplement pursues this analysis further. Essentially, during downswings,
improved performance is strongly correlated with predicing larger declines than
the AR model. The AR model tends to overpredict relative to the decline, but
adding indicators tends to diminish this effect. During upswings, the AR model
tends to underpredict, but when the addition of an indicator improves
performance (a more rare occurrence) it is rarely due to higher median
predictions.   

\subsection{Effects of Leading or Lagging Behavior}

As described in Section \ref{sec:signals-locations}, each of the indicators we
examine could be said to measure aspects on disease progression that would
precede a positive test. That is, we imagine that these signals should ``lead''
cases. It is entirely reasonable to imagine that, prior to an increase of
confirmed COVID-19 tests reported by public health in a particular location, we
would see an increase in insurance claims for COVID-19-related medical visits.
However, it may well be the case that this leading behavior is different during
different periods. In fact, we find that the ``leadingness'' of the indicators
is more pronounced during downswings and flat periods than during upswings, a
plausible explanation for the decreased performance noted above.

In order to measure the leadingness on an indicator, we use the cross
correlation function. Essentially this examines the Pearson correlation between a
shifted version of the indicator and the case rate. So if $X_{\ell,t}$ is
shifted forward by 7 days, and the magnitude of the correlation of that shifted series with
$Y_{\ell,t}$ (unshifted) is high, we would say that $X_{\ell}$ leads $Y_{\ell}$
by 7 days. This process is repeated for a range of shifts, forward and backward,
resulting in leadingness and laggingness scores. Further details are given in
the supplement.

Figure \ref{fig:leading-lagging} shows the correlations between the difference
in the WIS of the AR model relative to $F$ with the leading and laggingness
scores stratified by whether the target is classified as up, down, or flat. One
would expect that increases in $\WIS(AR) - \WIS(AR)$ would be positively
correlated with leadingness. This relationship is strongest during down periods
and weakest during up periods with flat in the middle. In fact, for each
indicator, the strength of correlations between improved performance and
leadingness can be ordered down $>$ flat $>$ up. The relationship with
laggingness is the opposite, though less pronounced. CHNG-CLI's forecast
performance doesn't correlate very strongly with laggingness while any
laggingness in Google-AA during down and flat periods results in realtively
worse forecasting performance.

Because a time series can, to a degree, both simultaneously leading and lagging,
we also examine the correlation between performance and the difference between
leadingness and laggingness. Figure \ref{fig:diff-btw-leading-lagging} shows
this behavior. Again, indicators that are more leading then lagging tend to
are positively correlated with improved relative performance. This relationship
is most pronounced during periods of decreasing case rates and attenuates in
other scenarios.






\begin{figure}[t]
  \includegraphics[width=\columnwidth]{fig/leading-and-lagging-1.pdf}
  \caption{Correlation of the difference in WIS with the ``leadingness'' and
    ``laggingness'' of the indicator at the target date.}
  \label{fig:leading-lagging}
\end{figure}


\begin{figure}[t]
  \includegraphics[width=\columnwidth]{fig/diff-in-lead-lag-1.pdf}
  \caption{Correlation of the difference in WIS with difference between the
    ``leadingness'' and ``laggingness'' of the indicator at the target date.}
  \label{fig:diff-btw-leading-lagging}
\end{figure}




\section{Discussion}

Can auxiliary indicators improve COVID-19 forecasting and hotspot prediction
models?  Our answer, based on analyzing five auxiliary indicators from the  
COVIDcast API (defined using from medical insurance claims, internet-based
surveys, and Google search trends) is undoubtedly ``yes''. However, there are 
levels of nuance to such an answer that must be explained.  None of the
indicators that we have investigated appear to be the ``silver bullet'' that 
one might have hoped for, revolutionizing the tractability of the prediction 
problem, rendering it easy when it was once hard (in the absence of auxiliary 
information).  Rather, the gains in accuracy from the indicator models (over an
autoregressive model based only on past case rates) appear to be nontrivial, and
consistent across modes of analysis, but modest.  In forecasting, the indicator 
models appear to be most useful in periods in which case rates are flat or
trending down, rather than periods in which case rates are trending up (as one 
might hope to see is the benefit provided by a hypothetical ``leading
indicator'').    

As mentioned previously, the indicator models could likely be improved by using 
location-specific effects, as well as using nowcasting techniques to estimate
finalized indicator values before using them as features (in order to account
for backfill in the claims-based signals in particular).  Beyond this, it is
certainly possible that more sophisticated models for forecasting or hotspot
prediction would lead to different results, and even different
insights. Natural directions to explore include using interactions, using
multiple indicators in a single model, and leveraging HRR demographics or
mobility patterns.  That said, we are doubtful that more sophisticated modeling
techniques would change the ``topline'' conclusion---that auxiliary indicators 
can provide nontrivial, consistent, but modest gains in forecasting and
hotspot prediction. Whether a more sophisticated model would be able to leverage
the indicators in such a way as to change some of the finer conclusions---e.g.,
by offering clear improvements in periods in which cases are trending up---is 
less clear to us. 

Reiterate importance of AS OF

Not the end of the story.  Should we even be forecasting case rates?  More
careful models---combine hotspots and forecasting?

[  Finish by reiterating the importance of vintage data.  Emphasizing the rigor
or our approach, how we have provided a template for others to be able to do the
same thing with new models ]

% RJT: some text commented out below.  May be useful for finishing off the
% discussion   

% This paper is as much about demonstrating how one might go about answering
% such questions about feature importance in forecasting and hotspot prediction 
% general, as providing an empirically rigorous answer to this precise question
% in the context of the COVID-19 pandemic. 

% We conclude by observing that the approach we have taken in this paper is
% somewhat opposite to that of much of the COVID-19 forecasting literature.  We
% have chosen to consider only very simple forecasting models while devoting
% most of our effort to accounting for as much of the complexity of the data and 
% evaluation as possible.  By contrast, many papers focus on very complicated
% forecasting approaches but then evaluate them under unrealistic, retrospective 
% conditions.

\showmatmethods{} % Display the Materials and Methods section

\acknow{Please include your acknowledgments here, set in a single paragraph. Please do not include any acknowledgments in the Supporting Information, or anywhere else in the manuscript.}

\showacknow{} % Display the acknowledgments section

% Bibliography
\bibliography{../../common/covidcast.bib}

\end{document}
