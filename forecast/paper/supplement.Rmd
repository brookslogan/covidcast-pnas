---
bibliography: [../../common/covidcast.bib, pnas-materials/pnas-sample.bib]
output: 
  pdf_document:
    keep_tex: true
    template: pnas-suppl-template.tex
params:
  fd_casefloor: 30
  flag_jumps: 15
---


<!-- PNAS materials -->

<!-- Comment out or remove this line before generating final copy for submission; this will also remove the warning re: "Consecutive odd pages found". -->
\instructionspage  



<!-- we need this -->
\maketitle

<!-- Adds the main heading for the SI text. Comment out this line if you do not have any supporting information text. -->

\SItext

All our text goes here. Reference figures with the R chuck label as Figure~\ref{fig:fcast-finalized}. 

All figures go on their own page after all of the text...

Edit `pnas-suppl-template.tex` for the correct Author list and Title.


# Examining the relative advantage of using finalized rather than vintage data

The goal of this section is to quantify the effect of
not properly accounting for the question of "what was known when" in
performing retrospective evaluations of forecasters.   Figures \ref{fig:fcast-finalized} and \ref{fig:hot-finalized} show
what Figures 3 and 4 in the main paper would have looked like if we
had simply trained all models using the finalized data rather than
using vintage data.  This comparison can be seen most
straightforwardly in Figures \ref{fig:fcast-honest-v-finalized} and \ref{fig:hot-honest-v-finalized}, which show the ratio in performance between the vintage and finalized
versions.  When methods are
given the finalized version of the data rather than the version available at the time that the forecast would have been made, all methods
appear (misleadingly) to have better performance than they would
actually have had if run prospectively.  For example, for forecasting case rates
7-days ahead, the WIS of all methods is at least 8\% larger than what would have been
recorded using the finalized values of the data.  This effect
diminishes as the forecasting horizon increases, reflecting the fact
that these forecasters rely less heavily on recent data than very
short-term forecasters.  Crucially, some methods are
"helped" more than others by the less scrupulous retrospective
evaluation, underscoring the difficulty of avoiding misleading
conclusions when performing retrospective evaluations of forecasters.

The \chngcli~indicator (along with the other claims-based signals) is the most
affected by this distinction, reflecting the latency in claims-based
reporting.  This supports the importance of efforts to provide "nowcasts" for claims signals
(which corresponds to a 0-ahead "forecast" of what the claims
signal's value will be once all data has been collected). Looking at
the \chngcli~and \dv~curves in Figure \ref{fig:fcast-finalized}, we
can see that they perform very similarly when trained on the finalized data.  This is reassuring
because they are in principle measuring the same thing (namely, the percentage of
  outpatient visits that are primarily about COVID-related symptoms).
  The substantial difference in their curves in Figure 3 of the
  main paper must therefore reflect their having very different
  backfill profiles.  
  
While \dv~is one of the least affected methods by using finalized
rather than vintage values, it is one of the most affected methods for the hotspot problem.
This is a reminder that the forecasting and hotspot problems are
distinct problems.  For example, the hotspot problem does not measure
the ability to distinguish between flat and downward trends.

Even the \ar~model is affected by this distinction, reflecting the
fact that the case rates themselves (i.e., the response values) are also
subject to revision.  The forecasters based on indicators are thus
affected both by revisions to the indicators and by revisions to the
case rates.  In the case of the \gs~model, in which we only used finalized
values for the \gs~indicator, the difference in performance can be
wholly attributed to revisions of case rates.




# Aggregating with geometric mean

In this section, we consider using the geometric mean instead of the
arithmetic mean when aggregating the weighted interval score (WIS) across location-time pairs.
There are three reasons why the geometric mean may be desirable.
\begin{enumerate}
\item  WIS is right-skewed, being bounded below by zero and having
  occasional very large values.   Figure \ref{fig:wis-densities} illustrates that the
  densities appear roughly log-Gaussian.  The geometric mean is a
  natural choice in such a context since the relative ordering of forecasters is
  determined by the arithmetic mean of the {\em logarithm} of their WIS
  values.
\item In the main paper, we report the ratio of the mean WIS of a
  forecaster to the mean WIS of the baseline forecaster. Another
  choice could be to take the mean of the ratio of WIS values for the
  two methods. (This latter choice would penalize a method less for
  doing poorly where the baseline forecaster also does poorly.)
Using instead the geometric mean makes the order of aggregation and
scaling immaterial since the ratio of geometric means is the same as
the geometric mean of ratios.
\item If one imagines that a forecaster's WIS is composed of
  multiplicative space-time effects $S_{\ell,t}$ shared across all forecasters,
  i.e. $\WIS(F_{\ell,t,f},Y_{\ell,t})=S_{\ell,t}E_{f}$, then
  taking the ratio of two forecasters' geometric mean WIS values will
  effectively cancel these space-time effects.
\end{enumerate}

Figure \ref{fig:fcast-adjusted} uses the geometric
mean for aggregation.  Comparing this with Figure 3 of the main paper,
we see that the main conclusions are largely unchanged; however,
\chngcli~now does appear better than \ar.  This behavior would be
expected if \chngcli's poor performance is attributable to a
relatively small number of large errors (as opposed to a large number
of moderate errors).  Indeed, Figure 5 of the main paper further
corroborates this, in which we see the heaviest left tails occuring
for \chngcli.

# Bootstrap results

As explained in Section 2.B. of the main paper, a (somewhat cynical)
hypothesis for why we see benefits in forecasting and hotspot
prediction is that the indicators are not actually providing useful
information but they are instead simply acting as a sort of implicit
regularization,"  leading to shrinkage on the autoregressive
coefficients, leading to less volatile predictions.  To investigate
this hypothesis, we consider fitting  "noise features" that in truth
should have zero coefficients.  Recall (from the main paper) that at each forecast date, we
train a model on 6,426 location-time pairs.  Indicator models are
based on six features, corresponding to the three autoregressive terms
and the three lagged indicator values.  To form noise features
we resample from other rows.  In particular, at each location $\ell$ and
time $t$, we replace the triplet $(X_{\ell,t}, X_{\ell,t-7},
X_{\ell,t-14})$ by the triplet $(X_{\ell^*,t^*}, X_{\ell^*,t^*-7},
X_{\ell^*,t^*-14})$, where $(\ell^*,t^*)$ is a location-time pair
sampled with replacement from the 6,426 location-time pairs.  Figures
\ref{fig:fcast-booted}--\ref{fig:hot-booted} show the results.  No
method exhibits a noticeable performance gain over the \ar~method,
leading us to dismiss the implicit regularization hypothesis.

\attn{Would be good for someone to double check that the bootstrap
  procedure is correctly described.}

<!--

# Correlations with lagged actuals

Alden's histograms are in Figure~\ref{fig:cor-wis-ratio} and Figure~\ref{fig:cor-wis-ratio-m1}.

-->

# Upswings and Downswings

In this section we provide extra details about the upswing / flat / downswing 
analysis described in the main text. Figure \ref{fig:upswing-summary} shows
the overall results, examining the average difference $\WIS(AR) - \WIS(F)$ in 
period. Figure \ref{fig:hotspots-upswing-downswing} shows the same information
for the hotspot task. On average, during downswings and flat periods, the 
indicator-assisted models have lower classification error and higher 
log likelihood than the AR model. For hotspots, both Google-AA and CTIS-CLIIC
perform better than the AR model during upswings, in contrast to the forecasting
task, where only Google-AA improves. For a related analysis, Figure 
\ref{fig:cor-wis-ratio} shows histograms of the Spearman
correlation (Spearman's $\rho$, a rank-based measure of association) between
the $\WIS(F)/\WIS(AR)$ and the magnitude of the swing. Again we see that case
rate increases are positively related to diminished performance of the 
indicator models.

One hypothesis for diminished relative performance during upswings is that 
the AR model tends to overpredict downswings and underpredict upswings. Adding
indicators seems to help avoid this behavior on the downswing but not as much 
on upswings. Figure \ref{fig:upswing-corr-table} shows the correlation between
between $\WIS(AR) - \WIS(F)$ and the difference of their median forecasts. During
downswings, this correlation is large, implying that improved relative performance
of $F$ is related to making lower forecasts than the AR model. The opposite is
true during upswings. This is largely to be expected. However, the relationship
attenuates in flat periods and during upswings. That is, when performance is better
in those cases, it may be due to other factors than simply making predictions
in the correct direction, for example, narrower confidence intervals.


# Leadingness and laggingness

Currently, both figures are in the manuscript. Probably just need text here.

# Examining data in 2021

In this section, we investigate the sensitivity of the results to the
period over which we train and evaluate the models.  In the main
paper, we end all evaluation on December 31, 2020.  Figures
\ref{fig:fcast-alldates} -- \ref{fig:hot-alldates} show how the
results would differ if we extended this analysis through March
  31, 2021. Comparing Figure \ref{fig:fcast-alldates} to Figure 3 of
the main paper, one sees that as ahead increases most methods now improve
relative to the baseline forecaster. When compared to other methods, 
\chngcli~appears much better than
it had previously; however, all forecasters other than \chngcov~and
\dv~are performing less well relative to the baseline than before.
These changes are likely due to the differing nature of the pandemic
in 2021, with flat and downward trends much more common than upward
trajectories.  Indeed, the nature of the hotspot prediction problem is
quite different in this period.  With a 21-day training window, it is
common for there to be very few hotspots in training.

# Deprecated

There are a few blocks at the bottom (figures with Google symptoms only and the
old trajectory plots) that we can remove once we decide.








```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.width = 6.5, fig.height = 4.5,
                      fig.align = "center",
                      fig.path = "fig/",
                      cache = TRUE, 
                      out.width = "\\textwidth",
                      autodep = TRUE,
                      message = FALSE, 
                      warning = FALSE)
```

```{r source-funs, cache=FALSE}
library(scales)
library(cowplot)
library(covidcast)
library(tidyverse)
library(lubridate)
library(evalcast)

source("../code/eval_funs.R")
source("../code/hotspot_funs.R")

path_to_data <- "~/Documents/COVID-Delphi/fcast-indicators-paper/temp_data/"
```

```{r data-loading}
actuals <- readRDS(file.path(path_to_data, "actuals.RDS"))
fcasts_honest <- readRDS(file.path(path_to_data, "results_honest.RDS")) %>%
  process_res_cases(actuals) %>%
  filter(forecaster != "gs_inherit") 
fcasts_finalized <- readRDS(file.path(path_to_data, "results_dishonest.RDS")) %>%
  process_res_cases(actuals) %>%
  filter(forecaster != "gs_inherit")
hotspots_honest <- readRDS(file.path(path_to_data, "hotspots_honest.RDS")) %>%
  process_res_hotspots() %>%
  filter(forecaster != "gs_inherit")
hotspots_finalized <- readRDS(
  file.path(path_to_data, "hotspots_dishonest.RDS")) %>%
  process_res_hotspots() %>%
  filter(forecaster != "gs_inherit")
cases7dav <- readRDS(file.path(path_to_data, "cases7dav.RDS")) %>%
  rename(case_num = value)
if (params$fd_casefloor >= 0) {
  fcasts_honest <- filter_case_floor(fcasts_honest, cases7dav, params)
  fcasts_finalized <- filter_case_floor(fcasts_finalized, cases7dav, params)
}
if (params$flag_jumps > 0) {
  fcasts_honest <- filter_jumps(fcasts_honest, actuals, params)
  fcasts_finalized <- filter_jumps(fcasts_finalized, actuals, params)
  hotspots_honest <- filter_jumps(hotspots_honest, actuals, params)
  hotspots_finalized <- filter_jumps(hotspots_finalized, actuals, params)
}
```

```{r gs-processing}
# only honest
fcasts_gs <- intersect_averagers(
  fcasts_honest, "forecaster", 
  c("geo_value", "ahead", "forecast_date", "target_end_date")) %>%
  filter(forecaster != "gs_subset") %>%
  mutate(forecaster = recode(forecaster, gs = "Google-AA"))
fcasts_honest <- filter(fcasts_honest, forecaster != "gs_subset") %>%
  mutate(forecaster = recode(forecaster, gs = "Google-AA"))
fcasts_finalized <- filter(fcasts_finalized, forecaster != "gs_subset")%>%
  mutate(forecaster = recode(forecaster, gs = "Google-AA"))
hotspots_gs <- intersect_averagers(
  hotspots_honest %>% filter(!is.na(value), !is.na(actual)), 
  "forecaster", 
  c("geo_value", "ahead", "forecast_date", "target_end_date")) %>%
  filter(forecaster != "gs_subset") %>%
  mutate(forecaster = recode(forecaster, gs = "Google-AA"))
hotspots_honest <- intersect_averagers( # Deal with DV NaN's
  hotspots_honest %>% 
    filter(! is.na(value), !is.na(actual), forecaster != "gs_subset"), 
  "forecaster", 
  c("geo_value", "ahead", "forecast_date", "target_end_date")) %>%
  mutate(forecaster = recode(forecaster, gs = "Google-AA"))
hotspots_finalized <- intersect_averagers( 
  hotspots_finalized %>% 
    filter(! is.na(value), !is.na(actual), forecaster != "gs_subset"), 
  "forecaster", 
  c("geo_value", "ahead", "forecast_date", "target_end_date")) %>%
  mutate(forecaster = recode(forecaster, gs = "Google-AA"))
```

```{r hrr-names}
hrr_names <- read_csv(file.path(path_to_data, "Hospital_Referral_Regions.csv"), 
                      col_types = "_ccc__") %>%
  mutate(state = substr(hrrcity, 1, 2),
         hrrname = paste(HRR_lbl, state)) %>%
  select(hrrnum, hrrname)
hrr_tab <- pull(hrr_names, hrrname)
names(hrr_tab) <- pull(hrr_names, hrrnum)
```



<!-- All figures and tables below this line -->



<!-- Finalized v. vintage -->

```{r fcast, include=FALSE}
# Figure 3 in the paper
plotter(fcasts_honest %>% filter(period != "jm"), 
        "wis", Mean, scaler = "strawman_wis", 
        order_of_operations = c("aggr","scale")) +
  ylab("Mean WIS (relative to baseline)")
```

```{r hot, include=FALSE}
# Figure 4 in the manuscript
plotter_hotspots(hotspots_honest %>% filter(period != "jm")) + 
  geom_hline(yintercept = 0.5)
```




```{r fcast-finalized, fig.cap="Forecasting performance using finalized data. Compare to Figure 3 in the manuscript."}
plotter(fcasts_finalized %>% filter(period != "jm"), 
        "wis", Mean, scaler = "strawman_wis", 
        order_of_operations = c("aggr","scale")) +
  ylab("Mean WIS (relative to baseline)")
```


\clearpage


```{r hot-finalized, fig.cap="Hotspot prediction performance using finalized data. Compare to Figure 4 in the manuscript."}
plotter_hotspots(hotspots_finalized %>% filter(period != "jm")) +
  geom_hline(yintercept = 0.5)
```

\clearpage


```{r fcast-honest-v-finalized, fig.cap="Relative forecast WIS with vintage compared to finalized data. Using finalized data leads to overly optimistic performance."}
plotter(
  left_join(
    fcasts_honest %>% filter(period != "jm") %>% select(forecaster:wis),
    fcasts_finalized %>% filter(period != "jm") %>% select(forecaster:wis) %>%
      rename(finalized_wis = wis)
    ), 
  "wis", Mean, scaler = "finalized_wis", 
  order_of_operations = c("aggr","scale")) +
  geom_hline(yintercept = 1) +
  ylab("Mean WIS (vintage / finalized)")

```

\clearpage

```{r hot-honest-v-finalized, fig.cap="Relative AUC with vintage compared to finalized data. Using finalized data leads to overly optimistic hotspot performance."}
left_join(
  hotspots_honest %>%
    group_by(forecaster, ahead) %>%
    summarise(auc_honest = auc(value, actual)),
  hotspots_finalized %>%
    group_by(forecaster, ahead) %>%
    summarise(auc_finalized = auc(value, actual))) %>%
  ggplot(aes(ahead, auc_honest / auc_finalized, color = forecaster)) + 
  geom_line() + 
  geom_point() +
  theme_bw() + 
  scale_color_manual(values = fcast_colors, guide = guide_legend(nrow=1)) +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  geom_hline(yintercept = 1) +
  ylab("AUC (vintage / finalized)")
```




\clearpage



<!-- Geometric mean justifications -->


```{r wis-densities, fig.cap="Weighted interval score appears to more closely resemble a log-Gaussian distribution."}
ggplot(fcasts_honest %>% filter(period != "jm"), 
       aes(wis, fill = forecaster)) +
  geom_density() +
  scale_x_log10() +
  theme_bw() +
  xlab("WIS") +
  scale_fill_manual(values = fcast_colors) +
  facet_wrap(~forecaster) +
  theme(legend.position = "none")
```

\clearpage

```{r fcast-adjusted, fig.cap="Relative forecast performance using vintage data and summarizing with the more robust geometric mean."}
plotter(fcasts_honest %>% filter(period != "jm"), 
        "wis", GeoMean, scaler = "strawman_wis", 
        order_of_operations = c("scale","aggr")) +
  ylab("Geometric mean of WIS (relative to baseline)")
```


\clearpage


<!-- Bootstrap stuff -->

```{r bootstrap-loading}
fcasts_booted <- readRDS(
  file.path(path_to_data,"results_honest_bootstrapped.RDS")) %>%
  process_res_cases(actuals) %>%
  filter(forecaster != "gs_inherit")
if (params$fd_casefloor >= 0) {
  fcasts_booted <- filter_case_floor(fcasts_booted, cases7dav, params)
}
hotspots_booted <- readRDS(
  file.path(path_to_data, "hotspots_honest_bootstrapped.RDS")) %>%
  process_res_hotspots() %>%
  filter(forecaster != "gs_inherit")
if (params$flag_jumps > 0) {
  hotspots_booted <- filter_jumps(hotspots_booted, actuals, params)
  fcasts_booted <- filter_jumps(fcasts_booted, actuals, params)
}
fcasts_booted <- filter(fcasts_booted, forecaster != "gs_subset") %>%
  mutate(forecaster = recode(forecaster, gs = "Google-AA"))
hotspots_booted <- intersect_averagers( # Deal with DV NaN's
  hotspots_booted %>% 
    filter(! is.na(value), !is.na(actual), forecaster != "gs_subset"), 
  "forecaster", 
  c("geo_value", "ahead", "forecast_date", "target_end_date")) %>%
  mutate(forecaster = recode(forecaster, gs = "Google-AA"))
```

```{r fcast-booted, fig.cap = "Forecast performance when indicators are replaced with samples from their empirical distribution. Performance is largely similar to the AR model."}
plotter(fcasts_booted %>% filter(period != "jm"), 
        "wis", Mean, scaler = "strawman_wis", 
        order_of_operations = c("aggr","scale")) +
  ylab("Mean WIS (relative to baseline)")
```

\clearpage

```{r fcast-booted-adjusted, fig.cap = "Forecast performance as measured with the geometric mean when indicators are replaced with samples from their empirical distribution. Performance is largely similar to the AR model."}
plotter(fcasts_booted %>% filter(period != "jm"), 
        "wis", GeoMean, scaler = "strawman_wis", 
        order_of_operations = c("scale","aggr")) +
  ylab("Geometric mean of WIS (relative to baseline)")
```

\clearpage

```{r hot-booted, fig.cap = "Hotspot prediction performance when indicators are replaced with samples from their empirical distribution. Performance is largely similar to the AR model."}
plotter_hotspots(hotspots_booted %>% filter(period != "jm")) + 
  geom_hline(yintercept = 0.5)
```



\clearpage












<!-- Upswings vs. downswings -->

```{r up-down-processing}
preds <- readRDS(file.path(path_to_data, "predictions_honest.RDS"))
preds <- preds %>% 
  filter(abs(quantile - 0.5) < 1e-6) %>% 
  select(-quantile) %>%
  mutate(forecaster = recode(forecaster,
                             AR3 = "AR",
                             AR3CHCLI3 = "CHNG-CLI",
                             AR3CHCOV3 = "CHNG-COVID",
                             AR3DVCLI3 = "DV-CLI",
                             AR3FBCLI3 = "CTIS-CLIIC",
                             AR3GSSAA3_Subset = "gs_subset",
                             AR3GSSAA3_Zero = "Google-AA",
                             Baseline = "strawman"
  )) %>%
  filter(! forecaster %in% c("gs_subset", "strawman"))

up_down <- actuals %>%
  arrange(geo_value, target_end_date) %>%
  group_by(geo_value) %>%
  left_join(cases7dav, c("target_end_date" = "time_value", "geo_value")) %>%
  mutate(pct_change = (actual - lag(actual,7)) / lag(actual,7),
         udf = case_when(
           pct_change >= .25 ~ "up",
           pct_change <= -.25 ~ "down",
           TRUE ~ "flat")) %>%
  filter(case_num > 30, !is.na(pct_change)) #%>%
  #rename(forecast_date = target_end_date)

corr_df <- left_join(fcasts_honest, preds, 
                     by = c("geo_value", "ahead", "forecaster", 
                            "forecast_date")) %>%
  select(-actual_fd, -starts_with("strawman"), -ae) %>%
  rename(med = value)
rm(preds)

up_down_df <- left_join(corr_df %>% 
                  ungroup() %>%
                  filter(forecaster != "AR"),
                corr_df %>% 
                  ungroup() %>%
                  filter(forecaster == "AR") %>%
                  select(-forecaster, -actual_td) %>%
                  rename(AR_wis = wis, AR_med = med)) %>%
  inner_join(up_down, by = c("geo_value","target_end_date"))
```

```{r upswing-summary, fig.cap="Average difference between the WIS of the AR model and the WIS of the other forecasters. The indicator-assisted forecasters do best during down and flat periods."}
# Figure 5 in the manuscript
up_down_df_summary <- up_down_df %>%
  filter(period != "jm") %>%
  group_by(forecaster, udf) %>%
  summarise(mean = Mean(AR_wis - wis))
up_down_df_summary %>%
  ggplot(aes(udf, mean , fill = forecaster)) +
  geom_bar(width = 0.6, position = position_dodge(width=0.6),
           stat = "identity") +
  scale_fill_manual(values = fcast_colors, guide = guide_legend(nrow = 1)) +
  scale_y_continuous() +
  theme_bw() +
  geom_hline(yintercept = 0) +
  ylab("Mean of WIS(AR) - WIS(F)") +
  xlab("") +
  theme(legend.position = "bottom", legend.title = element_blank())
```  
  

```{r upswing-histogram, include=FALSE}
up_down_df %>% 
  filter(period != "jm") %>%
  group_by(forecaster, udf) %>%
  ggplot(aes(AR_wis - wis, fill = forecaster)) +
  geom_histogram(bins = 100) +
  facet_grid(udf ~ forecaster) +
  theme_bw() +
  ylab("Count") +
  theme(legend.position = "none") +
  scale_fill_manual(values = fcast_colors) +
  scale_y_log10(breaks = c(10,1000,100000), 
                labels = trans_format("log10", math_format(10^.x))) +
  xlab("AR WIS - forecaster WIS") +
  geom_vline(xintercept = 0)
```

```{r upswing-histogram-logged, eval = FALSE, fig.cap="Not sure if we want this here. Similar to Figure 5 in the manuscript but taking logs. "}
up_down_df %>% 
  filter(period != "jm") %>%
  group_by(forecaster, udf) %>%
  ggplot(aes((AR_wis + 1) / (wis + 1), fill = forecaster)) +
  geom_histogram(bins = 100) +
  facet_grid(udf ~ forecaster) +
  theme_bw() +
  ylab("Count") +
  theme(legend.position = "none") +
  scale_fill_manual(values = fcast_colors) +
  scale_x_log10() +
  scale_y_log10(breaks = c(10,1000,100000), 
                labels = trans_format("log10", math_format(10^.x))) +
  xlab("AR WIS - forecaster WIS") +
  geom_vline(xintercept = 1)
```

\clearpage


```{r hotspots-upswing-downswing, fig.cap="Classification and loglikelihood separated into periods of upswing, downswing, and flat cases. Like the analysis of the forecasting task in the main paper (see Figure 7), performance is better during down and flat periods."}
hot_udf <- inner_join(
  hotspots_honest %>% filter(period != "jm"),
  up_down %>% select(geo_value, target_end_date, udf))

cutoff <- mean(hot_udf %>% filter(forecaster == "AR") %>% pull(actual))

con_tab <- hot_udf %>% 
  filter(!is.na(actual)) %>%
  mutate(pred = value > cutoff) %>%
  group_by(forecaster, udf) %>%
  summarise(m = mean(pred != actual)) %>%
  ungroup()

con_tab <- left_join(
  con_tab %>% filter(forecaster != "AR"),
  con_tab %>% filter(forecaster == "AR") %>%
    select(-forecaster) %>%
    rename(mar = m))

llike_tab <- hot_udf %>%
  filter(!is.na(actual)) %>%
  mutate( # kill prob 0-1 predictions
    value = case_when(
      value < 1e-8 ~ 1e-8,
      value > 1-1e-8 ~ 1-1e-8,
      TRUE ~ value
    ),
    llike = (actual == 1) * log(value) + (actual == 0) * log(1 - value)) %>%
  group_by(forecaster, udf) %>%
  summarise(m = mean(llike)) %>%
  ungroup()

llike_tab <- left_join(
  llike_tab %>% filter(forecaster != "AR"),
  llike_tab %>% filter(forecaster == "AR") %>%
    select(-forecaster) %>%
    rename(mar = m))

bind_rows(con_tab %>% mutate(err = "Classification error"), 
          llike_tab %>% mutate(err = "Log likelihood")) %>%
  ggplot(aes(udf, (m - mar) / abs(mar) , fill = forecaster)) +
  geom_bar(width = 0.6, position = position_dodge(width=0.6),
           stat = "identity") +
  scale_fill_manual(values = fcast_colors, guide = guide_legend(nrow = 1)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw() +
  geom_hline(yintercept = 0) +
  facet_wrap(~err) +
  ylab("Change relative to AR") +
  xlab("") +
  theme(legend.position = "bottom", legend.title = element_blank())
```


\clearpage


<!-- Correlations with lagged actuals -->

```{r cor-with-actuals}
comparison_df <- left_join(
  fcasts_honest %>% filter(forecaster != "AR", period != "jm"),
  fcasts_honest %>% filter(forecaster == "AR", period != "jm") %>%
    select(-c(forecaster,target_end_date)) %>%
    rename(ar3_wis = wis),
  by = c("geo_value", "ahead", "forecast_date")) %>%
  group_by(forecaster, forecast_date, geo_value) %>%
  summarize(wis_ratio = mean(wis) / mean(ar3_wis), .groups = "drop") 

pct_chng_df <- readRDS(file.path(
  path_to_data, 
  "jhu-csse_confirmed_7dav_incidence_num_2021-05-18.RDS")) %>%
  pct_change(n = 7) %>%
  select(geo_value, time_value, pct_change) %>%
  filter(time_value %in% unique(comparison_df$forecast_date))

comparison_df <- comparison_df %>% 
  left_join(pct_chng_df, 
            by = c("forecast_date" = "time_value", "geo_value"))

pct_chng_df <- comparison_df %>% 
  group_by(forecaster, geo_value) %>%
  summarize(
    cor_wisratio_pctchange = cor(wis_ratio, pct_change, method = "spearman"),
    cor_abs_wisratio_minus_1_pctchange = cor(
      abs(wis_ratio - 1), pct_change,method = "spearman"), .groups = "drop")
```

```{r cor-wis-ratio, fig.cap="Histograms of the Spearman correlation between the ratio of AR to AR WIS with the percent change in smoothed case rates relative to 7 days earlier."}
ggplot(pct_chng_df %>% 
         group_by(forecaster) %>% 
         mutate(median = median(cor_wisratio_pctchange)), 
       aes(x = cor_wisratio_pctchange)) +
  geom_histogram(aes(y = ..count.. / sum(..count..), fill = forecaster), bins = 50) +
  scale_fill_manual(values = fcast_colors) +
  theme_bw() +
  theme(legend.position = "none") +
  geom_vline(aes(xintercept = median), linetype = "dotted") + 
  geom_vline(xintercept = 0) +
  facet_wrap(~forecaster) + 
  xlab("Spearman correlation") + ylab("Relative frequency") +
  scale_y_continuous(labels = scales::label_percent(accuracy = .1))
```

\clearpage

```{r cor-wis-ratio-m1, eval = FALSE, fig.cap="This is Alden's second set of histograms. Here we have the correlation of the absolute value of WIS ratio - 1 with the percent change in 7dav cases relative to 7 days earlier"}
ggplot(pct_chng_df %>% 
         group_by(forecaster) %>% 
         mutate(median = median(cor_abs_wisratio_minus_1_pctchange)), 
       aes(x = cor_abs_wisratio_minus_1_pctchange)) +
  geom_histogram(aes(y = ..count.. / sum(..count..), fill = forecaster), bins = 50) +
  scale_fill_manual(values = fcast_colors) +
  theme_bw() +
  theme(legend.position = "none") +
  geom_vline(aes(xintercept = median), linetype = "dotted") + 
  geom_vline(xintercept = 0) +
  facet_wrap(~forecaster) + 
  xlab("Spearman correlation") + ylab("Relative frequency") +
  scale_y_continuous(labels = scales::label_percent(accuracy = 1))
```

\clearpage


```{r upswing-corr-table, fig.cap="Correlation of the difference in WIS with the difference in median predictions for the AR model relative to the indicator-assisted forecaster. In down periods, improvements in forecast risk are highlycorrelated with lower median predictions. The opposite is true in up periods. This suggests, as one might expect that improved performance of the indicator-assisted model is attributable to being closer to the truth then the AR model. This conclusion is stronger in down periods then in up periods."}
up_down_df %>% 
  filter(period != "jm") %>% 
  group_by(forecaster, udf) %>%
  summarise(cor = cor(AR_wis - wis, AR_med - med)) %>%
  ggplot(aes(udf, cor , fill = forecaster)) +
  geom_bar(width = 0.6, position = position_dodge(width=0.6),
           stat = "identity") +
  scale_fill_manual(values = fcast_colors, guide = guide_legend(nrow = 1)) +
  theme_bw() +
  geom_hline(yintercept = 0) +
  theme(legend.position = "bottom", legend.title = element_blank())

# up_down_df %>% 
#   filter(period != "jm") %>% 
#   group_by(forecaster, udf) %>%
#   summarise(cor = cor(AR_wis - wis, AR_med - med)) %>%
#   pivot_wider(names_from = forecaster, values_from = cor) %>%
#   kableExtra::kbl(
#     booktabs = TRUE, digits = 2, centering = TRUE,
#     caption = paste("Correlation of the difference in WIS between the AR",
#                    "model with the difference in median predictions. In down",
 #                   "periods, improvements in forecast risk are highly",
#                    "correlated with lower median predictions. The opposite",
#                    "is true in up periods. This suggests, as one might expect",
#                    "that improved performance of the indicator-assisted",
#                    "model is attributable to being closer to the truth then",
#                    "the AR model. This conclusion is stronger in down",
#                    "periods then in up periods."))
```

\clearpage



<!-- Leadingness and laggingness -->

```{r lead-lag-processing}
# see lead-lag-analysis.R for signal downloading and processing
lead_lag <- function(x, y, lag.max = NULL, na.len = 14) {
  if (sum(is.na(x)) >= na.len || sum(is.na(y)) >= na.len) {
    return(c(leading = NA, lagging = NA))
  }
  out <- ccf(x, y, lag.max = lag.max, plot = FALSE, na.action = na.omit)
  clim <- qnorm((1 + .95) / 2) / sqrt(out$n.used)
  lag_vec <- drop(out$lag)
  cc <- drop(out$acf)
  cc <- cc * (cc > clim)
  return(c(leading = mean(cc[lag_vec < 0]),
           lagging = mean(cc[lag_vec > 0])))
}

slide_lead_lag <- function(x, y, min_len = 56, lag.max = NULL, na.len = 14) {
  n <- length(x)
  outlist <- list()
  idx <- 1:min_len
  for (i in min_len:n) {
    outlist[[i]] <- lead_lag(x[idx], y[idx], lag.max, na.len)
    idx <- idx + 1
  }
  outlist <- bind_rows(outlist)
  bind_rows(
    tibble(leading = rep(NA, min_len-1), lagging = rep(NA, min_len -1)),
    outlist)
}

all_sigs <- readRDS(file.path(
  path_to_data, "all_signals_wide_as_of_2020-05-15.RDS"))

all_sigs_imputed <- all_sigs %>% 
  group_by(geo_value) %>% 
  arrange(time_value) %>%
  mutate(across(contains("value+0"), 
                ~zoo::na.locf(.x, na.rm = FALSE, maxgap = 13))) %>%
  pivot_longer(contains("value+0"))

sigs_cor <- left_join(
  all_sigs_imputed %>% filter(! str_detect(name, "jhu-csse")),
  all_sigs_imputed %>% filter(str_detect(name, "jhu-csse")) %>%
    select(-name) %>%
    rename(cases = value))

lead_lag_metric <- sigs_cor %>% 
  group_by(geo_value, name) %>%
  arrange(time_value) %>%
  group_modify(~ {
    bind_cols(time_value = .x$time_value, slide_lead_lag(.x$value, .x$cases))
  })

lead_lag_metric <- lead_lag_metric %>%
  mutate(
    forecaster = recode(
      name,
      `value+0:chng_smoothed_adj_outpatient_cli` = "CHNG-CLI",
      `value+0:chng_smoothed_adj_outpatient_covid` = "CHNG-COVID",
      `value+0:doctor-visits_smoothed_adj_cli` = "DV-CLI",
      `value+0:fb-survey_smoothed_hh_cmnty_cli` = "CTIS-CLIIC",
      `value+0:google-symptoms_sum_anosmia_ageusia_smoothed_search` = "Google-AA")) %>%
  ungroup() %>%
  select(-name)

df2 <- left_join(
  up_down_df, 
  lead_lag_metric,
  by = c("geo_value","forecaster","target_end_date" = "time_value"))
```

```{r leading-and-lagging, include=FALSE}
# Figure 6 in the manuscript
df2 %>%
  filter(period != "jm") %>%
  group_by(forecaster, udf) %>%
  summarise(
    leadingness = cor(AR_wis - wis, leading, use = "pairwise.complete.obs"),
    laggingness = cor(AR_wis - wis, lagging, use = "pairwise.complete.obs")) %>%
  pivot_longer(leadingness:laggingness) %>%
  ggplot(aes(udf, value, fill = forecaster)) +
  geom_bar(width = 0.6, position = position_dodge(width=0.6),
           stat = "identity") +
  geom_hline(yintercept = 0) +
  scale_fill_manual(values = fcast_colors) +
  facet_wrap(~name) +
  theme_bw() +
  ylab("Correlation") +
  xlab("") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

```{r diff-in-lead-lag, include=FALSE}
# Figure 7 in the manuscript
df2 %>%
  filter(period != "jm") %>%
  group_by(forecaster, udf) %>%
  summarise(cor = cor(AR_wis - wis, leading - lagging, 
                      use = "pairwise.complete.obs")) %>%
  ggplot(aes(udf, cor, fill = forecaster)) +
  geom_bar(width = 0.6, position = position_dodge(width=0.6),
           stat = "identity") +
  geom_hline(yintercept = 0) +
  scale_fill_manual(values = fcast_colors) +
  theme_bw() +
  ylab("Correlation") +
  xlab("") +
  theme(legend.position = "bottom", legend.title = element_blank())
```



<!-- Examining data in 2021 -->

```{r fcast-alldates, fig.cap="Forecast performance over all periods. Performance largely improves for all forecasters with the inclusion of data in 2021."}
plotter(fcasts_honest,
        "wis", Mean, scaler = "strawman_wis", 
        order_of_operations = c("aggr","scale")) +
  ylab("Mean WIS (relative to baseline)")
```

\clearpage

```{r fcast-alldates-adjusted, fig.cap="Forcast performance over all periods aggregaged with the geometric mean. Again, the inclusion of data in 2021 leads to improved performance."}
plotter(fcasts_honest, 
        "wis", GeoMean, scaler = "strawman_wis", 
        order_of_operations = c("scale","aggr")) +
  ylab("Geometric mean of WIS (relative to baseline)")
```

\clearpage

```{r hot-alldates, fig.cap="Area under the curve for hotspot predictions including data in 2021. Performance degrades relative to the period in 2020. However, there are far fewer hotspots during this period as case rates declined in much of the country."}
plotter_hotspots(hotspots_honest)
```






\clearpage




<!-- Figures in the manuscript in their own scripts -->


```{r revisions-dv-jhu, eval=FALSE}
# Figure 1 in the manuscript
source("../code/revisions.R")
```





```{r ny-trajectory, eval=FALSE}
# Figure 2 in the manuscript
source("../code/ny-trajectory.R")
gg
```


```{r ccf-dv-finalized, eval=FALSE}
# Figure ?? in the manuscript
source("../code/ccf-dv-finalized.R")
gg
```





<!-- All code below is deprecated -->


```{r fcast-gs-locations, eval=FALSE}
plotter(fcasts_gs %>% filter(period != "jm"),
        "wis", Mean, scaler = "strawman_wis", 
        order_of_operations = c("aggr","scale")) +
  ylab("Mean WIS (relative to baseline)")
```

```{r fcast-gs-locations-adjusted, eval=FALSE}
plotter(fcasts_gs %>% filter(period != "jm"), 
        "wis", GeoMean, scaler = "strawman_wis", 
        order_of_operations = c("scale","aggr")) +
  ylab("Geometric mean of WIS (relative to baseline)")
```

```{r hot-gs-locations, eval=FALSE}
plotter_hotspots(hotspots_gs %>% filter(period != "jm")) +
  geom_hline(yintercept = 0.5)
```



```{r traj-data, eval = FALSE}
source("../code/trajectory_plot_funs.R")
preds <- readRDS(file.path(path_to_data, "predictions_honest.RDS"))
traj_best <- get_trajectory_plots(fcasts_honest, preds, actuals, hrr_tab, 
                                  "only2020", "best")
traj_worst <- get_trajectory_plots(fcasts_honest, preds, actuals, hrr_tab,
                                   "only2020", "worst")
rm(preds)
```

```{r trajectory-plots, eval = FALSE}
for (nm in names(traj_best)) print(trajectory_panel(nm, traj_best, traj_worst))
```


\clearpage


<!-- PNAS says: Add this line AFTER all your figures and tables -->
\FloatBarrier



<!-- More PNAS example material -->
<!-- Probably remove all this -->

\movie{Type legend for the movie here.}


\dataset{dataset_one.txt}{Type or paste legend here.}

\dataset{dataset_two.txt}{Type or paste legend here. Adding longer text to show what happens, to decide on alignment and/or indentations for multi-line or paragraph captions.}

